{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 加载数据项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_table('../../round1_ijcai_18_train_20180301.txt',sep=' ')\n",
    "test_df = pd.read_table('../../round1_ijcai_18_test_a_20180301.txt',sep=' ')\n",
    "\n",
    "# 线下线上数据统一进行特征处理\n",
    "test_df['is_trade'] = -1\n",
    "total_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 时间处理: 分离天, 星期几, 上中下午/晚上, 小时数\n",
    "# date最终不使用，直接用day(第 0 - 7 天)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "def extract_date(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "def extract_weekday(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.weekday()\n",
    "def extract_hour(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.hour\n",
    "\n",
    "total_df['day'] = le.fit_transform(total_df['context_timestamp'].apply(lambda x: extract_date(x)))\n",
    "total_df['weekday'] = total_df['context_timestamp'].apply(lambda x: extract_weekday(x))\n",
    "total_df['hour'] = total_df['context_timestamp'].apply(lambda x: extract_hour(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 复杂类型特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df['item_category_list_len'] = train_df['item_category_list'].apply(lambda x: len(x.split(';')))\n",
    "# train_df['item_property_list_len'] = le.fit_transform(\\\n",
    "#             pd.cut(train_df['item_property_list']\\\n",
    "#                    .apply(lambda x: len(x.split(';'))), bins=10, retbins=False\\\n",
    "#                   )\\\n",
    "#                                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_icl_map ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x10c5f6a58>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/prehawkmac/anaconda3/lib/python3.6/site-packages/xgboost/core.py\", line 368, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_ipl_map ... \n",
      "processing predict_category_property ...\n",
      "processing item_property_list ...\n",
      "processing item_category_list ...\n"
     ]
    }
   ],
   "source": [
    "#简化list等复杂类型的结构\n",
    "#item_category_list全展开\n",
    "#item_property_list全展开取频率>0.05的数据\n",
    "#predict_category_property会计算跟item prop和cate的余弦相似度\n",
    "#具体是把两个list的数据拼成cate:-1和cate:prop两种方式拼成一个字符串再跟predict_category_property的数据计算相似度\n",
    "\n",
    "def get_icl_map(df):\n",
    "    print(\"get_icl_map ... \")\n",
    "    dfX = df.copy()\n",
    "    dfX = dfX['item_category_list'].str.split(';', expand=True)\n",
    "\n",
    "#     s = sorted(list(dfX[0].unique()) + list(dfX[1].unique()) + list(dfX[2].unique()))\n",
    "    m = {}\n",
    "    for i in dfX[0].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"1\"\n",
    "    \n",
    "    for i in dfX[1].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"2\"\n",
    "    \n",
    "    for i in dfX[2].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"3\"\n",
    "    return m\n",
    "\n",
    "def get_ipl_map(df):\n",
    "    print(\"get_ipl_map ... \")\n",
    "    df1 = df.copy()\n",
    "    dfX = df1.copy()['item_property_list'].str.split(';')\n",
    "    dfX = pd.DataFrame(dfX)\n",
    "    \n",
    "    m = collections.defaultdict(float)\n",
    "    idx = 0\n",
    "    for _, row in dfX.iterrows():\n",
    "        for i in row[0]:\n",
    "            m[i] += 1\n",
    "    \n",
    "    ll = len(dfX)\n",
    "    for k,v in m.items():\n",
    "        m[k] = v / ll\n",
    "    return m\n",
    "\n",
    "def process_complex_types(dfX, icl_map, ipl_map):\n",
    "    def filter_unless_cate(arr):\n",
    "        ret = []\n",
    "        for i in arr:\n",
    "            if i in icl_map:\n",
    "                ret.append(i)\n",
    "        if len(ret) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def filter_unless_prop(arr):\n",
    "        ret = []\n",
    "        for i in arr:\n",
    "            freq = ipl_map.get(i, 0.)\n",
    "            if freq > 0.05:\n",
    "                ret.append(i)\n",
    "        if len(ret) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    #{cate}:-1命中则为1分\n",
    "    #{cate}:{prop}命中则为2分\n",
    "    #后期优化权重\n",
    "    def inner_product_recall_items(line):\n",
    "        line = line.split(\"|\")\n",
    "        item_category_list = line[0].split(\";\")\n",
    "        item_property_list = line[1].split(\";\")\n",
    "        \n",
    "        whole_combines = {}\n",
    "        for cate in item_category_list:\n",
    "            tmp = cate+\":\"+\"-1\"\n",
    "            whole_combines[tmp] = 1\n",
    "            for prop in item_property_list:\n",
    "                tmp = cate+\":\"+prop\n",
    "                whole_combines[tmp] = 2\n",
    "        \n",
    "        \n",
    "                \n",
    "        predict_category_property = line[2].split(\";\")\n",
    "        product = 0.\n",
    "        item_vec_len = math.sqrt(len(whole_combines))\n",
    "        user_vec_len = math.sqrt(len(predict_category_property))\n",
    "        for item in predict_category_property:\n",
    "            #x1 == 1\n",
    "            #y1 == 1\n",
    "            #x1*y1 == 1\n",
    "            #x2 == 0\n",
    "            #y2 == 1\n",
    "            #x2*y2 == 0\n",
    "            #所以product由x决定 += 1/0\n",
    "            product += whole_combines.get(item, 0)\n",
    "        \n",
    "        return product/(item_vec_len*user_vec_len)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"processing predict_category_property ...\")\n",
    "#     dfX['predict_category_property'] = dfX['predict_category_property'].str.split(';').map(lambda x: [i.split(\":\")[0] for i in x]).map(filter_unless_cate)\n",
    "    \n",
    "    dfX['predict_richness'] =  dfX['predict_category_property'].map(lambda x: 0 if len(x.strip()) == 0 else len(x.split(\";\")))\n",
    "    dfX['predict_category_property'] = dfX['item_category_list']+\"|\"+dfX['item_property_list']+\"|\"+dfX['predict_category_property']\n",
    "    dfX['predict_category_property'] = dfX['predict_category_property'].map(inner_product_recall_items)\n",
    "    \n",
    "    print(\"processing item_property_list ...\")\n",
    "    dfX['item_property_richness'] = dfX['item_property_list'].map(lambda x: 0 if len(x.strip()) == 0 else len(x.split(\";\")))\n",
    "    dfX['item_property_list'] = dfX['item_property_list'].str.split(';').map(filter_unless_prop)\n",
    "    \n",
    "    print(\"processing item_category_list ...\")\n",
    "    dfX['item_category_list'] = dfX['item_category_list'].str.split(';')\n",
    "    \n",
    "#     print \"generating item_category_list01 - item_category_list03 ...\"\n",
    "#     dfX['item_category_list01'] = dfX['item_category_list'].map(lambda x:x[0] if x != None and len(x) > 0 else None)\n",
    "#     dfX['item_category_list02'] = dfX['item_category_list'].map(lambda x:x[1] if x != None and len(x) > 1 else None)\n",
    "#     dfX['item_category_list03'] = dfX['item_category_list'].map(lambda x:x[2] if x != None and len(x) > 2 else None)\n",
    "    \n",
    "    return dfX\n",
    "\n",
    "\n",
    "\n",
    "# aaa = process_complex_types(train_df.copy(), get_icl_map(train_df), get_ipl_map(train_df))\n",
    "\n",
    "total_df = process_complex_types(total_df, get_icl_map(total_df), get_ipl_map(total_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转化属性增加trade_rate + trade_pv（按前一天）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计信息附加\n",
    "\n",
    "def calcTVTransform(df, key, key_y, filter_src, filter_dst, smoothing = 10, mean0=None):\n",
    "    if mean0 is None:\n",
    "        #计算目标的平均值做平缓用\n",
    "        mean0 = df.ix[filter_src, key_y].mean()\n",
    "        print(\"mean0:\", mean0)\n",
    "    \n",
    "    #取出key的所有值\n",
    "    df['_key1'] = df[key].astype('category').values.codes\n",
    "    \n",
    "    \n",
    "    #取出用于计算的源（后面聚合掉就没有顺序可言了）\n",
    "    df_key1_y = df.ix[filter_src, ['_key1', key_y]]\n",
    "    \n",
    "    #根据key的取值去聚合key_y的总数和总和，用户计算rate和count\n",
    "    grp1 = df_key1_y.groupby(['_key1'])\n",
    "    sum1 = grp1[key_y].aggregate(np.sum)\n",
    "    cnt1 = grp1[key_y].aggregate(np.size)\n",
    "    \n",
    "    vn_sum = 'sum_' + key\n",
    "    vn_cnt = 'cnt_' + key\n",
    "    \n",
    "    #取出dst（带序列）的所有key\n",
    "    v_codes = df.ix[filter_dst, '_key1']\n",
    "    \n",
    "    #得到_sum,_cnt，按dst的序列\n",
    "    _sum = sum1[v_codes].values\n",
    "    _cnt = cnt1[v_codes].values\n",
    "    _cnt[np.isnan(_sum)] = 0    \n",
    "    _sum[np.isnan(_sum)] = 0\n",
    "    \n",
    "    r = {}\n",
    "    r['exp'] = (_sum + smoothing * mean0)/(_cnt + smoothing)\n",
    "    r['cnt'] = _cnt\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prehawkmac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.020020953646445547\n",
      "mean0: 0.019681098532376536\n",
      "mean0: 0.01936040475528975\n",
      "mean0: 0.019312068989733002\n",
      "mean0: 0.018867648350361546\n",
      "mean0: 0.017276071305058635\n",
      "mean0: 0.016910189651869526\n"
     ]
    }
   ],
   "source": [
    "#计算前一天的交易率set到下一天，第0天用回自己\n",
    "# tmp = train_df.copy()\n",
    "tmp = total_df\n",
    "\n",
    "add_count = False\n",
    "# window = 2\n",
    "\n",
    "exp = \"exp_d_\"\n",
    "cnt = \"cnt_d_\"\n",
    "\n",
    "\n",
    "exp_numerical = {}\n",
    "cnt_numerical = {}\n",
    "\n",
    "#此处应该处理按天为梯度的数据\n",
    "for k in ['item_price_level',\n",
    "'item_sales_level',\n",
    "'item_collected_level',\n",
    "'item_pv_level',\n",
    "'user_gender_id',\n",
    "'user_age_level',\n",
    "'user_occupation_id',\n",
    "'user_star_level',\n",
    "'context_page_id',\n",
    "'shop_review_num_level',\n",
    "'shop_star_level']:\n",
    "    exp_k = exp+k\n",
    "    cnt_k = cnt+k\n",
    "    for day in range(0,8):\n",
    "#         start_d = max(day - window, 0)\n",
    "#         end_d = max(day - 1,0)\n",
    "        cal_day = max(day - 1, 0)\n",
    "        set_day = day\n",
    "\n",
    "        # print(\"cal %s trade_rate cnt %s set to %s\" % (k,cal_day, set_day))\n",
    "        \n",
    "        #start_d - day(不含day)用于计算，结果赋值到day上\n",
    "        days1 = (tmp.day.values == cal_day)\n",
    "        days2 = (tmp.day.values == set_day)\n",
    "        ret = calcTVTransform(tmp, k, 'is_trade', days1, days2)\n",
    "            \n",
    "        tmp.loc[tmp.day.values == day, exp_k] = ret[\"exp\"]\n",
    "        \n",
    "        exp_numerical[exp_k]=1\n",
    "        if add_count:\n",
    "            cnt_numerical[cnt_k]=1\n",
    "            tmp.loc[tmp.day.values == day, cnt_k] = ret[\"cnt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exp_d_item_price_level', 'exp_d_item_sales_level', 'exp_d_item_collected_level', 'exp_d_item_pv_level', 'exp_d_user_gender_id', 'exp_d_user_age_level', 'exp_d_user_occupation_id', 'exp_d_user_star_level', 'exp_d_context_page_id', 'exp_d_shop_review_num_level', 'exp_d_shop_star_level']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "exp_numerical = list(exp_numerical.keys())\n",
    "cnt_numerical = list(cnt_numerical.keys())\n",
    "\n",
    "print(exp_numerical)\n",
    "print(cnt_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标记训练用的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#标记训练数据\n",
    "\n",
    "target=\"is_trade\"\n",
    "                          \n",
    "                          \n",
    "                        \n",
    "categorical=[  \n",
    "#                 'shop_id',\n",
    "#                 'item_brand_id',\n",
    "#                 'item_city_id',\n",
    "                'item_price_level',\n",
    "                'item_sales_level',\n",
    "                'item_collected_level',\n",
    "                'item_pv_level',\n",
    "                'user_gender_id',\n",
    "                'user_age_level',\n",
    "                'user_occupation_id',\n",
    "                'user_star_level',\n",
    "                'context_page_id',\n",
    "                'shop_review_num_level',\n",
    "                'shop_star_level',\n",
    "                \"weekday\",\n",
    "                \"hour\"]\n",
    "\n",
    "numerical=[     'shop_review_positive_rate',\n",
    "                'shop_score_service',\n",
    "                'shop_score_delivery',\n",
    "                'shop_score_description',\n",
    "                'predict_category_property',\n",
    "                'predict_richness',\n",
    "                'item_property_richness'\n",
    "\n",
    "          ]#+exp_numerical\n",
    "listype = [\n",
    "    'item_property_list', \n",
    "    'item_category_list'\n",
    "]\n",
    "\n",
    "class filter_on_cols:\n",
    "    def __init__(self, target, categorical, numerical, listype):\n",
    "        self.target = target\n",
    "        self.categorical = categorical\n",
    "        self.numerical = numerical\n",
    "        self.listype = listype\n",
    "        \n",
    "\n",
    "    \n",
    "#     def get_raw_simple_cols(self):\n",
    "#         return [self.target]+self.categorical+self.numerical\n",
    "    def get_raw_target_col(self):\n",
    "        return self.target\n",
    "    def get_raw_categorical_cols(self):\n",
    "        return self.categorical\n",
    "    \n",
    "    def get_raw_numerical_cols(self):\n",
    "        return self.numerical\n",
    "    def get_raw_listype_cols(self):\n",
    "        return self.listype\n",
    "    \n",
    "    def get_onehoted_cols(self, t, df):\n",
    "        if t == \"listype\":\n",
    "            org_cols = self.listype\n",
    "        \n",
    "        if t == \"categorical\":\n",
    "            org_cols = self.categorical\n",
    "            \n",
    "        ret = []\n",
    "        for org_col in org_cols:\n",
    "            for cur_col in list(df.columns):\n",
    "                if cur_col.find(\"*ONEHOT*_\") != -1 and cur_col.find(org_col) != -1:\n",
    "                    ret.append(cur_col)\n",
    "        \n",
    "        return ret\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "filter_on_cols = filter_on_cols(target, categorical, numerical, listype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平滑处理连续型数据最后对复杂类型做onehot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据的预处理\n",
    "#double系列(例如好评率)的数据存在一个问题，没有考虑到评论量，所以乘了一个评论level，类似 好评率*评论数(分桶了) = 好评量(分桶了)\n",
    "#fillempty把-1设置成-0.01，配合mov2pos使用\n",
    "#mov2pos把有<0的数据都集体-min，如果一个数据是0-1，有-1出现的时候，上面设置成了-0.01，所以这列数据真实是-0.01 - 1,然后我会集体-(-0.01)\n",
    "#norm用了最大值最小值norm\n",
    "%run FeatureProcess.py\n",
    "\n",
    "total_df = total_df.copy()\n",
    "featProc = FeatureProcess(target=target, categorical=categorical, numerical=numerical, listype = listype)\n",
    "\n",
    "total_df[\"shop_review_num_level\"] = total_df[\"shop_review_num_level\"]\n",
    "total_df[\"shop_review_positive_rate\"] = total_df[\"shop_review_positive_rate\"]*total_df[\"shop_review_num_level\"]\n",
    "total_df[\"shop_score_service\"] = total_df[\"shop_score_service\"]*total_df[\"shop_review_num_level\"]\n",
    "total_df[\"shop_score_delivery\"] = total_df[\"shop_score_delivery\"]*total_df[\"shop_review_num_level\"]\n",
    "total_df[\"shop_score_description\"] = total_df[\"shop_score_description\"]*total_df[\"shop_review_num_level\"]\n",
    "\n",
    "total_df = featProc.fillempty(total_df, -0.01)\n",
    "total_df = featProc.mov2pos(total_df)\n",
    "total_df = featProc.norm(total_df)\n",
    "# train_df[filter_on_cols.get_raw_numerical_cols()].describe()\n",
    "\n",
    "\n",
    "# train_df[filter_on_cols.get_raw_numerical_cols()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加id分布穿越特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对有意义的id进行分桶\n",
    "# item_city_id, item_brand_id, shop_id, item_id\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "def bucker(se, bins, col):\n",
    "    \"\"\"获得某一列的分桶标识\"\"\"\n",
    "    df = pd.DataFrame(pd.cut(se, bins=bins, retbins=False))\n",
    "    df.columns = [col]\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "# 先构造统计表格\n",
    "tr_city_counts = bucker(train_df['item_city_id'].value_counts(), 10, 'item_city_id_buck')\n",
    "te_city_counts = bucker(test_df['item_city_id'].value_counts(), 10, 'item_city_id_buck')\n",
    "\n",
    "tr_brand_counts = bucker(train_df['item_brand_id'].value_counts(), 10, 'item_brand_id_buck')\n",
    "te_brand_counts = bucker(test_df['item_brand_id'].value_counts(), 10, 'item_brand_id_buck')\n",
    "\n",
    "tr_shop_counts = bucker(train_df['shop_id'].value_counts(), 10, 'shop_id_buck')\n",
    "te_shop_counts = bucker(test_df['shop_id'].value_counts(), 10, 'shop_id_buck')\n",
    "\n",
    "tr_item_counts = bucker(train_df['item_id'].value_counts(), 10, 'item_id_buck')\n",
    "te_item_counts = bucker(test_df['item_id'].value_counts(), 10, 'item_id_buck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用到原始df\n",
    "try:\n",
    "    del total_df['item_city_id_buck'], total_df['item_brand_id_buck'], total_df['shop_id_buck'], total_df['item_id_buck']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "total_df = total_df.join(tr_city_counts, on='item_city_id')\n",
    "total_df = total_df.join(tr_brand_counts, on='item_brand_id')\n",
    "total_df = total_df.join(tr_shop_counts, on='shop_id')\n",
    "total_df = total_df.join(tr_item_counts, on='item_id')\n",
    "\n",
    "\n",
    "total_df = total_df.join(te_city_counts, on='item_city_id', rsuffix='_drop')\n",
    "total_df = total_df.join(te_brand_counts, on='item_brand_id', rsuffix='_drop')\n",
    "total_df = total_df.join(te_shop_counts, on='shop_id', rsuffix='_drop')\n",
    "total_df = total_df.join(te_item_counts, on='item_id', rsuffix='_drop')\n",
    "total_df = total_df.drop(['item_city_id_drop', 'item_brand_id_drop', 'shop_id_drop', 'item_id_drop'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toOneHot ...\n"
     ]
    }
   ],
   "source": [
    "#onehot没啥好说的，就是onehot，toOneHotList实现有点不同分开了而已\n",
    "total_df = featProc.toOneHot(total_df)\n",
    "\n",
    "\n",
    "# train_df = featProc.toOneHotList(train_df)\n",
    "\n",
    "# train_df = featProc.cacheRun(featProc.toOneHot, train_df)\n",
    "# for c in train_df.columns:\n",
    "#     print(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指定需要用到的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478138,) (478138, 242) (478138,) (18371, 242)\n"
     ]
    }
   ],
   "source": [
    "train_df = total_df[total_df['is_trade']>=0]\n",
    "test_df = total_df[total_df['is_trade']<0]\n",
    "\n",
    "non_feat_columns = ['context_timestamp', 'instance_id', 'item_id', 'user_id', 'shop_id', 'item_brand_id', 'context_id',\n",
    "                   'item_city_id_buck', 'item_brand_id_buck', 'shop_id_buck', 'item_id_buck', \n",
    "                   'item_property_list', 'item_category_list', \n",
    "                   'is_trade']\n",
    "\n",
    "D = train_df['context_timestamp']\n",
    "X = train_df.drop(non_feat_columns, axis=1).values\n",
    "y = train_df[['is_trade']].values.ravel()\n",
    "\n",
    "X_online = test_df.drop(non_feat_columns, axis=1).values\n",
    "print(D.shape, X.shape, y.shape, X_online.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# from sklearn.model_selection import GridSearchCV as GSCV   #Perforing grid search\n",
    "# from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "\n",
    "%run ../util/time_series_split.py\n",
    "\n",
    "xgb = xgboost.XGBClassifier(n_jobs=5)\n",
    "\n",
    "m = None\n",
    "\n",
    "# 出离线结果, 留空最后一天数据\n",
    "# dtsv = DateTimeSplit(dateSeries=D, fmt=\"%Y-%m-%d\", n=1)\n",
    "# for train_i, test_i in dtsv.split(X, y):\n",
    "#     X_train, y_train = X[train_i], y[train_i]\n",
    "#     X_test, y_test = X[test_i], y[test_i]\n",
    "#     m = xgb.fit(X_train, y_train)\n",
    "#     print(log_loss(y_test, m.predict_proba(X_test)))\n",
    "\n",
    "# 不出离线结果, 全量数据\n",
    "m = xgb.fit(X, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出比赛格式\n",
    "result = pd.DataFrame()\n",
    "result['instance_id'] = test_df['instance_id']\n",
    "result['predicted_score'] = pd.DataFrame(m.predict_proba(X_online))[1].values\n",
    "result.to_csv('20180321_xgb.csv', sep = ' ', header=True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#这块一般不怎么需要改了，拿最后一天做验证\n",
    "#注意下面验证版和测试版，测试版是我们自己玩的，验证版是扔线上的，通过注释不同的跑就行了\n",
    "\n",
    "\n",
    "def ffea(on_cols):\n",
    "    ret = {}\n",
    "    for v in on_cols:\n",
    "        if v == \"user_occupation_id\":\n",
    "            continue\n",
    "        \n",
    "        ret[v] = 1\n",
    "    return ret.keys()\n",
    "\n",
    "# %run BaseFrame.py\n",
    "% run ../util/time_series_split.py\n",
    "\n",
    "tmp_train_df = train_df.copy().fillna(0)\n",
    "\n",
    "\n",
    "categorical_one_hot_cols = filter_on_cols.get_onehoted_cols(\"categorical\", tmp_train_df)\n",
    "listype_one_hot_cols = filter_on_cols.get_onehoted_cols(\"listype\", tmp_train_df)\n",
    "\n",
    "categorical_cols = filter_on_cols.get_raw_categorical_cols()\n",
    "listype_cols = filter_on_cols.get_raw_listype_cols()\n",
    "numerical_cols = filter_on_cols.get_raw_numerical_cols()\n",
    "target_col = filter_on_cols.get_raw_target_col()\n",
    "\n",
    "print(\"org dataset = %s\" % len(tmp_train_df))\n",
    "print(tmp_train_df[\"day\"].value_counts())\n",
    "\n",
    "d_train_indices = None\n",
    "\n",
    "class DateTimeSplit2:\n",
    "\n",
    "    def __init__(self, dataSeries, gap):\n",
    "        self.D = dataSeries \n",
    "        self.gap = gap\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return 1\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        # X, y = indexable(X, y)\n",
    "        train_indices = self.D[self.D['day']<self.gap].reset_index().index.values\n",
    "        test_indices  = self.D[self.D['day']==self.gap].reset_index().index.values\n",
    "        #print(train_indices)\n",
    "        #print(X[np.array(train_indices)].shape, X[np.array(test_indices)].shape)\n",
    "        yield np.array(train_indices), np.array(test_indices)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV as GSCV   #Perforing grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "\n",
    "\n",
    "X_train = tmp_train_df[on_cols].copy()\n",
    "del X_train['is_trade']\n",
    "y_train = tmp_train_df['is_trade'].copy()\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[0] == tmp_train_df.shape[0])\n",
    "\n",
    "dtsv = DateTimeSplit2(tmp_train_df, 6)\n",
    "score_func = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "parameters = {\n",
    "  'nthread':[7], #when use hyperthread, xgboost may become slower\n",
    "  'learning_rate': [0.1], #so called `eta` value\n",
    "  'max_depth': [5,6,7,8,9,10],\n",
    "  'n_estimators': [32,64,100],\n",
    "  'reg_lambda': [0]\n",
    "}\n",
    "\n",
    "xgb = xgboost.XGBClassifier()\n",
    "gs = GSCV(xgb, parameters, scoring=score_func, cv=dtsv)\n",
    "\n",
    "on_cols = [target_col,\"day\"]+categorical_cols+numerical_cols\n",
    "\n",
    "\n",
    "gs.fit(X_train.values, y_train.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
