{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhua/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.19.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import ffmyh\n",
    "import time\n",
    "import math\n",
    "# from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "# from fastFM.datasets import make_user_item_regression\n",
    "# from fastFM import als\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_org = pd.read_table('/Users/yuhua/先验CTR模型线上观察/Alimama比赛/round1_ijcai_18_train_20180301.txt',sep=' ')\n",
    "test_df_org = pd.read_table('/Users/yuhua/先验CTR模型线上观察/Alimama比赛/round1_ijcai_18_test_a_20180301.txt',sep=' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_org.columns\n",
    "\n",
    "train_df = train_df_org.copy().set_index(\"context_id\")\n",
    "test_df = test_df_org.copy().set_index(\"context_id\")\n",
    "\n",
    "test_df[\"is_trade\"] = -1\n",
    "train_df = train_df.append(test_df)\n",
    "# print len(train_df)\n",
    "# print train_df[[\"is_trade\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2018-09-24': 6, '2018-09-25': 7, '2018-09-20': 2, '2018-09-21': 3, '2018-09-22': 4, '2018-09-23': 5, '2018-09-19': 1, '2018-09-18': 0}\n"
     ]
    }
   ],
   "source": [
    "# 时间处理: 分离天, 星期几, 上中下午/晚上, 小时数\n",
    "# date最终不使用，直接用day(第 0 - 7 天)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "def extract_weekday(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.weekday()\n",
    "def extract_hour(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.hour\n",
    "\n",
    "train_df['date'] = train_df['context_timestamp'].apply(lambda x: extract_date(x))\n",
    "train_df['weekday'] = train_df['context_timestamp'].apply(lambda x: extract_weekday(x))\n",
    "train_df['hour'] = train_df['context_timestamp'].apply(lambda x: extract_hour(x))\n",
    "\n",
    "m = {}\n",
    "for idx, date in enumerate(sorted(train_df['date'].unique())):\n",
    "    m[date]=idx\n",
    "print(m)\n",
    "\n",
    "for idx, d in enumerate(sorted(train_df['date'].unique())):\n",
    "    train_df[\"day\"] = train_df['date'].map(lambda x: m[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拆解复杂类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_icl_map ... \n",
      "get_ipl_map ... \n",
      "processing predict_category_property ...\n",
      "processing item_property_list ...\n",
      "processing item_category_list ...\n",
      "generating item_category_1, item_category_2 ...\n"
     ]
    }
   ],
   "source": [
    "#简化list等复杂类型的结构\n",
    "#item_category_list全展开\n",
    "#item_property_list全展开取频率>0.05的数据\n",
    "#predict_category_property会计算跟item prop和cate的余弦相似度\n",
    "#具体是把两个list的数据拼成cate:-1和cate:prop两种方式拼成一个字符串再跟predict_category_property的数据计算相似度\n",
    "def get_icl_map(df):\n",
    "    print(\"get_icl_map ... \")\n",
    "    dfX = df.copy()\n",
    "    dfX = dfX['item_category_list'].str.split(';', expand=True)\n",
    "\n",
    "#     s = sorted(list(dfX[0].unique()) + list(dfX[1].unique()) + list(dfX[2].unique()))\n",
    "    m = {}\n",
    "    for i in dfX[0].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"1\"\n",
    "    \n",
    "    for i in dfX[1].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"2\"\n",
    "    \n",
    "    for i in dfX[2].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"3\"\n",
    "    return m\n",
    "\n",
    "def get_ipl_map(df):\n",
    "    print(\"get_ipl_map ... \")\n",
    "    df1 = df.copy()\n",
    "    dfX = df1.copy()['item_property_list'].str.split(';')\n",
    "    dfX = pd.DataFrame(dfX)\n",
    "    \n",
    "    m = collections.defaultdict(float)\n",
    "    idx = 0\n",
    "    for _, row in dfX.iterrows():\n",
    "        for i in row[0]:\n",
    "            m[i] += 1\n",
    "    \n",
    "    ll = len(dfX)\n",
    "    for k,v in m.items():\n",
    "        m[k] = v / ll\n",
    "    return m\n",
    "\n",
    "def process_complex_types(dfX, icl_map, ipl_map):\n",
    "    def filter_unless_cate(arr):\n",
    "        ret = []\n",
    "        for i in arr:\n",
    "            if i in icl_map:\n",
    "                ret.append(i)\n",
    "        if len(ret) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def filter_unless_prop(arr):\n",
    "        ret = []\n",
    "        for i in arr:\n",
    "            freq = ipl_map.get(i, 0.)\n",
    "            if freq > 0.05:\n",
    "                ret.append(i)\n",
    "        if len(ret) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def unique_list(arr):\n",
    "        return list(set(arr))\n",
    "    \n",
    "    #{cate}:-1命中则为1分\n",
    "    #{cate}:{prop}命中则为2分\n",
    "    #后期优化权重\n",
    "    def inner_product_recall_items(line):\n",
    "        line = line.split(\"|\")\n",
    "        item_category_list = unique_list(line[0].split(\";\"))\n",
    "        item_property_list = unique_list(line[1].split(\";\"))\n",
    "        \n",
    "        whole_combines = {}\n",
    "        for cate in item_category_list:\n",
    "            tmp = cate+\":\"+\"-1\"\n",
    "            whole_combines[tmp] = 1\n",
    "            for prop in item_property_list:\n",
    "                tmp = cate+\":\"+prop\n",
    "                whole_combines[tmp] = 2\n",
    "        \n",
    "        \n",
    "                \n",
    "        predict_category_property = unique_list(line[2].split(\";\"))\n",
    "        product = 0.\n",
    "        item_vec_len = math.sqrt(len(whole_combines))\n",
    "        user_vec_len = math.sqrt(len(predict_category_property))\n",
    "        for item in predict_category_property:\n",
    "            #x1 == 1\n",
    "            #y1 == 1\n",
    "            #x1*y1 == 1\n",
    "            #x2 == 0\n",
    "            #y2 == 1\n",
    "            #x2*y2 == 0\n",
    "            #所以product由x决定 += 1/0\n",
    "            product += whole_combines.get(item, 0)\n",
    "        \n",
    "        return product/(item_vec_len*user_vec_len)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"processing predict_category_property ...\")\n",
    "#     dfX['predict_category_property'] = dfX['predict_category_property'].str.split(';').map(lambda x: [i.split(\":\")[0] for i in x]).map(filter_unless_cate)\n",
    "    \n",
    "    dfX['predict_richness'] =  dfX['predict_category_property'].map(lambda x: 0 if len(x.strip()) == 0 else len(x.split(\";\")))\n",
    "    dfX['predict_category_property'] = dfX['item_category_list']+\"|\"+dfX['item_property_list']+\"|\"+dfX['predict_category_property']\n",
    "    dfX['predict_category_property'] = dfX['predict_category_property'].map(inner_product_recall_items)\n",
    "    \n",
    "    print(\"processing item_property_list ...\")\n",
    "    dfX['item_property_richness'] = dfX['item_property_list'].map(lambda x: 0 if len(x.strip()) == 0 else len(x.split(\";\")))\n",
    "    dfX['item_property_list'] = dfX['item_property_list'].str.split(';').map(filter_unless_prop).map(unique_list)\n",
    "    \n",
    "    print(\"processing item_category_list ...\")\n",
    "    dfX['item_category_list'] = dfX['item_category_list'].str.split(';')\n",
    "    \n",
    "    print \"generating item_category_1, item_category_2 ...\"\n",
    "#     dfX['item_category_list01'] = dfX['item_category_list'].map(lambda x:x[0] if x != None and len(x) > 0 else None)\n",
    "    dfX['item_category_1'] = dfX['item_category_list'].map(lambda x:x[1] if x != None and len(x) > 1 else None)\n",
    "    dfX['item_category_2'] = dfX['item_category_list'].map(lambda x:x[2] if x != None and len(x) > 2 else None)\n",
    "    \n",
    "    return dfX\n",
    "\n",
    "\n",
    "\n",
    "# aaa = process_complex_types(train_df.copy(), get_icl_map(train_df), get_ipl_map(train_df))\n",
    "\n",
    "train_df = process_complex_types(train_df, get_icl_map(train_df), get_ipl_map(train_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转化属性增加trade_rate + trade_pv（按前一天）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计信息附加\n",
    "\n",
    "def calcTVTransform(df, key, key_y, filter_src, filter_dst, smoothing = 10, mean0=None):\n",
    "    if mean0 is None:\n",
    "        #计算目标的平均值做平缓用\n",
    "        mean0 = df.ix[filter_src, key_y].mean()\n",
    "        print(\"mean0:\", mean0)\n",
    "    \n",
    "    #取出key的所有值\n",
    "    df['_key1'] = df[key].astype('category').values.codes\n",
    "    \n",
    "    \n",
    "    #取出用于计算的源（后面聚合掉就没有顺序可言了）\n",
    "    df_key1_y = df.ix[filter_src, ['_key1', key_y]]\n",
    "    \n",
    "    #根据key的取值去聚合key_y的总数和总和，用户计算rate和count\n",
    "    grp1 = df_key1_y.groupby(['_key1'])\n",
    "    sum1 = grp1[key_y].aggregate(np.sum)\n",
    "    cnt1 = grp1[key_y].aggregate(np.size)\n",
    "    \n",
    "    vn_sum = 'sum_' + key\n",
    "    vn_cnt = 'cnt_' + key\n",
    "    \n",
    "    #取出dst（带序列）的所有key\n",
    "    v_codes = df.ix[filter_dst, '_key1']\n",
    "    \n",
    "    #得到_sum,_cnt，按dst的序列\n",
    "    _sum = sum1[v_codes].values\n",
    "    _cnt = cnt1[v_codes].values\n",
    "    _cnt[np.isnan(_sum)] = 0    \n",
    "    _sum[np.isnan(_sum)] = 0\n",
    "    \n",
    "    r = {}\n",
    "    r['exp'] = (_sum + smoothing * mean0)/(_cnt + smoothing)\n",
    "    r['cnt'] = _cnt\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算前一天的交易率set到下一天，第0天用回自己\n",
    "# tmp = train_df.copy()\n",
    "tmp = train_df\n",
    "\n",
    "add_count = False\n",
    "# window = 2\n",
    "\n",
    "exp = \"exp_d_\"\n",
    "cnt = \"cnt_d_\"\n",
    "\n",
    "\n",
    "exp_numerical = {}\n",
    "cnt_numerical = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal day trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhua/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal day trade_rate cnt 1 set to 1\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal day trade_rate cnt 2 set to 2\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal day trade_rate cnt 3 set to 3\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal day trade_rate cnt 4 set to 4\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal day trade_rate cnt 5 set to 5\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal day trade_rate cnt 6 set to 6\n",
      "('mean0:', 0.016910189651869526)\n"
     ]
    }
   ],
   "source": [
    "tmp = train_df\n",
    "k = \"day\"\n",
    "exp_k = \"exp_d_day\"\n",
    "for day in xrange(0,7):\n",
    "    cal_day = day\n",
    "    set_day = day\n",
    "\n",
    "    print(\"cal %s trade_rate cnt %s set to %s\" % (k,cal_day, set_day))\n",
    "\n",
    "    #start_d - day(不含day)用于计算，结果赋值到day上\n",
    "    days1 = (tmp.day.values == cal_day)\n",
    "    days2 = (tmp.day.values == set_day)\n",
    "    ret = calcTVTransform(tmp, k, 'is_trade', days1, days2, 0)\n",
    "\n",
    "    tmp.loc[tmp.day.values == day, exp_k] = ret[\"exp\"]\n",
    "\n",
    "    exp_numerical[exp_k]=1\n",
    "\n",
    "tmp.loc[tmp.day.values == 7, exp_k] = 0.0169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal item_price_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_price_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_price_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_price_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_price_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_price_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_price_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_price_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal item_sales_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_sales_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_sales_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_sales_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_sales_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_sales_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_sales_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_sales_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal item_collected_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_collected_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_collected_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_collected_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_collected_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_collected_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_collected_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_collected_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal item_pv_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_pv_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_pv_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_pv_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_pv_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_pv_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_pv_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_pv_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal user_gender_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_gender_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_gender_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal user_gender_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal user_gender_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal user_gender_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal user_gender_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal user_gender_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal user_age_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_age_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_age_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal user_age_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal user_age_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal user_age_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal user_age_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal user_age_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal user_occupation_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_occupation_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_occupation_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal user_occupation_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal user_occupation_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal user_occupation_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal user_occupation_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal user_occupation_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal user_star_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_star_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal user_star_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal user_star_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal user_star_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal user_star_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal user_star_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal user_star_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal context_page_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal context_page_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal context_page_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal context_page_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal context_page_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal context_page_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal context_page_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal context_page_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal shop_review_num_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal shop_review_num_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal shop_review_num_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal shop_review_num_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal shop_review_num_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal shop_review_num_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal shop_review_num_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal shop_review_num_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal shop_star_level trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal shop_star_level trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal shop_star_level trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal shop_star_level trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal shop_star_level trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal shop_star_level trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal shop_star_level trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal shop_star_level trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal item_brand_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_brand_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_brand_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_brand_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_brand_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_brand_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_brand_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_brand_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal item_city_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_city_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_city_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_city_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_city_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_city_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_city_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_city_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal item_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal item_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal item_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal item_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal item_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal item_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal item_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n",
      "cal shop_id trade_rate cnt 0 set to 0\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal shop_id trade_rate cnt 0 set to 1\n",
      "('mean0:', 0.020020953646445547)\n",
      "cal shop_id trade_rate cnt 1 set to 2\n",
      "('mean0:', 0.019681098532376536)\n",
      "cal shop_id trade_rate cnt 2 set to 3\n",
      "('mean0:', 0.01936040475528975)\n",
      "cal shop_id trade_rate cnt 3 set to 4\n",
      "('mean0:', 0.019312068989733002)\n",
      "cal shop_id trade_rate cnt 4 set to 5\n",
      "('mean0:', 0.018867648350361546)\n",
      "cal shop_id trade_rate cnt 5 set to 6\n",
      "('mean0:', 0.017276071305058635)\n",
      "cal shop_id trade_rate cnt 6 set to 7\n",
      "('mean0:', 0.016910189651869526)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#此处应该处理按天为梯度的数据\n",
    "for k in ['item_price_level',\n",
    "'item_sales_level',\n",
    "'item_collected_level',\n",
    "'item_pv_level',\n",
    "'user_gender_id',\n",
    "'user_age_level',\n",
    "'user_occupation_id',\n",
    "'user_star_level',\n",
    "'context_page_id',\n",
    "'shop_review_num_level',\n",
    "'shop_star_level',\n",
    "         \n",
    "\n",
    "\"item_brand_id\",\n",
    "\"item_city_id\",\n",
    "\"item_id\",\n",
    "\"shop_id\"\n",
    "         ]:\n",
    "    exp_k = exp+k\n",
    "    cnt_k = cnt+k\n",
    "    for day in xrange(0,8):\n",
    "#         start_d = max(day - window, 0)\n",
    "#         end_d = max(day - 1,0)\n",
    "        cal_day = max(day - 1, 0)\n",
    "        set_day = day\n",
    "\n",
    "        print(\"cal %s trade_rate cnt %s set to %s\" % (k,cal_day, set_day))\n",
    "        \n",
    "        #start_d - day(不含day)用于计算，结果赋值到day上\n",
    "        days1 = (tmp.day.values == cal_day)\n",
    "        days2 = (tmp.day.values == set_day)\n",
    "        ret = calcTVTransform(tmp, k, 'is_trade', days1, days2)\n",
    "            \n",
    "        tmp.loc[tmp.day.values == day, exp_k] = ret[\"exp\"]\n",
    "        \n",
    "        exp_numerical[exp_k]=1\n",
    "        if add_count:\n",
    "            cnt_numerical[cnt_k]=1\n",
    "            tmp.loc[tmp.day.values == day, cnt_k] = ret[\"cnt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exp_d_user_age_level', 'exp_d_item_pv_level', 'exp_d_shop_review_num_level', 'exp_d_item_sales_level', 'exp_d_user_occupation_id', 'exp_d_item_city_id', 'exp_d_item_brand_id', 'exp_d_user_star_level', 'exp_d_item_price_level', 'exp_d_day', 'exp_d_item_id', 'exp_d_shop_star_level', 'exp_d_item_collected_level', 'exp_d_user_gender_id', 'exp_d_context_page_id', 'exp_d_shop_id']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "exp_numerical = exp_numerical.keys()\n",
    "cnt_numerical = cnt_numerical.keys()\n",
    "\n",
    "print(exp_numerical)\n",
    "print(cnt_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标记训练用的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#标记训练数据\n",
    "import copy\n",
    "target=\"is_trade\"\n",
    "                          \n",
    "                          \n",
    "                        \n",
    "categorical=[  \n",
    "                'item_price_level',\n",
    "                'item_sales_level',\n",
    "                'item_collected_level',\n",
    "                'item_pv_level',\n",
    "                'user_gender_id',\n",
    "                'user_age_level',\n",
    "                'user_occupation_id',\n",
    "                'user_star_level',\n",
    "                'context_page_id',\n",
    "                'shop_review_num_level',\n",
    "                'shop_star_level',\n",
    "                \"weekday\",\n",
    "                \"hour\",\n",
    "    \n",
    "                'item_category_1',\n",
    "                'item_category_2',\n",
    "                \n",
    "                \"item_brand_id\",\n",
    "                \"item_city_id\",\n",
    "                \"item_id\",\n",
    "                \"shop_id\"\n",
    "            ]\n",
    "\n",
    "numerical=[     'shop_review_positive_rate',\n",
    "                'shop_score_service',\n",
    "                'shop_score_delivery',\n",
    "                'shop_score_description',\n",
    "                'predict_category_property',\n",
    "                'predict_richness',\n",
    "                'item_property_richness'\n",
    "                   \n",
    "#               ]\n",
    "          ]+exp_numerical\n",
    "\n",
    "listype = [\n",
    "    'item_property_list', \n",
    "    'item_category_list'\n",
    "]\n",
    "\n",
    "class filter_on_cols:\n",
    "    def __init__(self, target, categorical, numerical, listype):\n",
    "        self.target = target\n",
    "        self.categorical = categorical\n",
    "        self.numerical = numerical\n",
    "        self.listype = listype\n",
    "        \n",
    "\n",
    "    \n",
    "#     def get_raw_simple_cols(self):\n",
    "#         return [self.target]+self.categorical+self.numerical\n",
    "    def get_raw_target_col(self):\n",
    "        return copy.deepcopy(self.target)\n",
    "    def get_raw_categorical_cols(self):\n",
    "        return copy.deepcopy(self.categorical)\n",
    "    \n",
    "    def get_raw_numerical_cols(self):\n",
    "        return copy.deepcopy(self.numerical)\n",
    "    def get_raw_listype_cols(self):\n",
    "        return copy.deepcopy(self.listype)\n",
    "    \n",
    "    def get_onehoted_cols(self, t, df):\n",
    "        if t == \"listype\":\n",
    "            org_cols = self.listype\n",
    "        \n",
    "        if t == \"categorical\":\n",
    "            org_cols = self.categorical\n",
    "            \n",
    "        ret = []\n",
    "        for org_col in org_cols:\n",
    "            for cur_col in list(df.columns):\n",
    "                if cur_col.find(\"*ONEHOT*_\") != -1 and cur_col.find(org_col) != -1:\n",
    "                    ret.append(cur_col)\n",
    "        \n",
    "        return ret\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "filter_on_cols = filter_on_cols(target, categorical, numerical, listype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_review_positive_rate</th>\n",
       "      <th>shop_score_service</th>\n",
       "      <th>shop_score_delivery</th>\n",
       "      <th>shop_score_description</th>\n",
       "      <th>predict_category_property</th>\n",
       "      <th>predict_richness</th>\n",
       "      <th>item_property_richness</th>\n",
       "      <th>exp_d_user_age_level</th>\n",
       "      <th>exp_d_item_pv_level</th>\n",
       "      <th>exp_d_shop_review_num_level</th>\n",
       "      <th>exp_d_item_sales_level</th>\n",
       "      <th>exp_d_user_occupation_id</th>\n",
       "      <th>exp_d_item_city_id</th>\n",
       "      <th>exp_d_item_brand_id</th>\n",
       "      <th>exp_d_user_star_level</th>\n",
       "      <th>exp_d_item_price_level</th>\n",
       "      <th>exp_d_day</th>\n",
       "      <th>exp_d_item_id</th>\n",
       "      <th>exp_d_shop_star_level</th>\n",
       "      <th>exp_d_item_collected_level</th>\n",
       "      <th>exp_d_user_gender_id</th>\n",
       "      <th>exp_d_context_page_id</th>\n",
       "      <th>exp_d_shop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.994833</td>\n",
       "      <td>0.971131</td>\n",
       "      <td>0.970495</td>\n",
       "      <td>0.974870</td>\n",
       "      <td>0.104144</td>\n",
       "      <td>4.810191</td>\n",
       "      <td>34.475055</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.019207</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.019142</td>\n",
       "      <td>0.019143</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.019122</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.019553</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.019206</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.023607</td>\n",
       "      <td>0.023567</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.074287</td>\n",
       "      <td>1.795871</td>\n",
       "      <td>10.150529</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.007672</td>\n",
       "      <td>0.015398</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.018198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.992780</td>\n",
       "      <td>0.966360</td>\n",
       "      <td>0.965677</td>\n",
       "      <td>0.969282</td>\n",
       "      <td>0.052705</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.017565</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.018384</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.017904</td>\n",
       "      <td>0.018869</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.005889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972347</td>\n",
       "      <td>0.971579</td>\n",
       "      <td>0.978493</td>\n",
       "      <td>0.099015</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.019105</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>0.018838</td>\n",
       "      <td>0.019286</td>\n",
       "      <td>0.019032</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.017962</td>\n",
       "      <td>0.019564</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.019312</td>\n",
       "      <td>0.013477</td>\n",
       "      <td>0.018953</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.019397</td>\n",
       "      <td>0.019370</td>\n",
       "      <td>0.013666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977822</td>\n",
       "      <td>0.976966</td>\n",
       "      <td>0.983626</td>\n",
       "      <td>0.148888</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.020799</td>\n",
       "      <td>0.020019</td>\n",
       "      <td>0.021639</td>\n",
       "      <td>0.022023</td>\n",
       "      <td>0.019853</td>\n",
       "      <td>0.022691</td>\n",
       "      <td>0.024957</td>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.023361</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>0.025495</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>0.020534</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.020518</td>\n",
       "      <td>0.026924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.108061</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.037022</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>0.135569</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.046764</td>\n",
       "      <td>0.079541</td>\n",
       "      <td>0.020021</td>\n",
       "      <td>0.190919</td>\n",
       "      <td>0.085729</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>0.190764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shop_review_positive_rate  shop_score_service  shop_score_delivery  \\\n",
       "count              496509.000000       496509.000000        496509.000000   \n",
       "mean                    0.994833            0.971131             0.970495   \n",
       "std                     0.011875            0.023607             0.023567   \n",
       "min                    -1.000000           -1.000000            -1.000000   \n",
       "25%                     0.992780            0.966360             0.965677   \n",
       "50%                     1.000000            0.972347             0.971579   \n",
       "75%                     1.000000            0.977822             0.976966   \n",
       "max                     1.000000            1.000000             1.000000   \n",
       "\n",
       "       shop_score_description  predict_category_property  predict_richness  \\\n",
       "count           496509.000000              496509.000000     496509.000000   \n",
       "mean                 0.974870                   0.104144          4.810191   \n",
       "std                  0.024823                   0.074287          1.795871   \n",
       "min                 -1.000000                   0.000000          1.000000   \n",
       "25%                  0.969282                   0.052705          3.000000   \n",
       "50%                  0.978493                   0.099015          5.000000   \n",
       "75%                  0.983626                   0.148888          6.000000   \n",
       "max                  1.000000                   0.632456         14.000000   \n",
       "\n",
       "       item_property_richness  exp_d_user_age_level  exp_d_item_pv_level  \\\n",
       "count           496509.000000         496509.000000        496509.000000   \n",
       "mean                34.475055              0.019209             0.019185   \n",
       "std                 10.150529              0.003042             0.002614   \n",
       "min                  6.000000              0.001875             0.001393   \n",
       "25%                 29.000000              0.017565             0.017940   \n",
       "50%                 33.000000              0.019105             0.019256   \n",
       "75%                 39.000000              0.020799             0.020019   \n",
       "max                100.000000              0.027407             0.108061   \n",
       "\n",
       "       exp_d_shop_review_num_level  exp_d_item_sales_level  \\\n",
       "count                496509.000000           496509.000000   \n",
       "mean                      0.019207                0.019196   \n",
       "std                       0.003695                0.005910   \n",
       "min                       0.001763                0.001014   \n",
       "25%                       0.017704                0.015128   \n",
       "50%                       0.018838                0.019286   \n",
       "75%                       0.021639                0.022023   \n",
       "max                       0.066678                0.037022   \n",
       "\n",
       "       exp_d_user_occupation_id  exp_d_item_city_id  exp_d_item_brand_id  \\\n",
       "count             496509.000000       496509.000000        496509.000000   \n",
       "mean                   0.019197            0.019142             0.019143   \n",
       "std                    0.001739            0.007672             0.015398   \n",
       "min                    0.001875            0.000885             0.000453   \n",
       "25%                    0.018679            0.013796             0.008008   \n",
       "50%                    0.019032            0.016975             0.017962   \n",
       "75%                    0.019853            0.022691             0.024957   \n",
       "max                    0.022668            0.135569             0.162300   \n",
       "\n",
       "       exp_d_user_star_level  exp_d_item_price_level      exp_d_day  \\\n",
       "count          496509.000000           496509.000000  496509.000000   \n",
       "mean                0.019192                0.019122       0.018545   \n",
       "std                 0.002805                0.008289       0.001955   \n",
       "min                 0.001875                0.001889       0.010179   \n",
       "25%                 0.018384                0.014195       0.017276   \n",
       "50%                 0.019564                0.016366       0.019312   \n",
       "75%                 0.020863                0.023361       0.019681   \n",
       "max                 0.046764                0.079541       0.020021   \n",
       "\n",
       "       exp_d_item_id  exp_d_shop_star_level  exp_d_item_collected_level  \\\n",
       "count  496509.000000          496509.000000               496509.000000   \n",
       "mean        0.019553               0.019197                    0.019206   \n",
       "std         0.019332               0.003712                    0.002980   \n",
       "min         0.000565               0.001033                    0.001057   \n",
       "25%         0.006452               0.016709                    0.017904   \n",
       "50%         0.013477               0.018953                    0.019162   \n",
       "75%         0.025495               0.022549                    0.020534   \n",
       "max         0.190919               0.085729                    0.035166   \n",
       "\n",
       "       exp_d_user_gender_id  exp_d_context_page_id  exp_d_shop_id  \n",
       "count         496509.000000          496509.000000  496509.000000  \n",
       "mean               0.019201               0.019208       0.019002  \n",
       "std                0.001895               0.002703       0.018198  \n",
       "min                0.010130               0.004412       0.000468  \n",
       "25%                0.018869               0.018735       0.005889  \n",
       "50%                0.019397               0.019370       0.013666  \n",
       "75%                0.019800               0.020518       0.026924  \n",
       "max                0.023103               0.025987       0.190764  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[filter_on_cols.get_raw_numerical_cols()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平滑处理连续型数据最后对复杂类型做onehot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_review_positive_rate</th>\n",
       "      <th>shop_score_service</th>\n",
       "      <th>shop_score_delivery</th>\n",
       "      <th>shop_score_description</th>\n",
       "      <th>predict_category_property</th>\n",
       "      <th>predict_richness</th>\n",
       "      <th>item_property_richness</th>\n",
       "      <th>exp_d_user_age_level</th>\n",
       "      <th>exp_d_item_pv_level</th>\n",
       "      <th>exp_d_shop_review_num_level</th>\n",
       "      <th>exp_d_item_sales_level</th>\n",
       "      <th>exp_d_user_occupation_id</th>\n",
       "      <th>exp_d_item_city_id</th>\n",
       "      <th>exp_d_item_brand_id</th>\n",
       "      <th>exp_d_user_star_level</th>\n",
       "      <th>exp_d_item_price_level</th>\n",
       "      <th>exp_d_day</th>\n",
       "      <th>exp_d_item_id</th>\n",
       "      <th>exp_d_shop_star_level</th>\n",
       "      <th>exp_d_item_collected_level</th>\n",
       "      <th>exp_d_user_gender_id</th>\n",
       "      <th>exp_d_context_page_id</th>\n",
       "      <th>exp_d_shop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.971535</td>\n",
       "      <td>0.970906</td>\n",
       "      <td>0.975237</td>\n",
       "      <td>0.164666</td>\n",
       "      <td>0.293092</td>\n",
       "      <td>0.302926</td>\n",
       "      <td>0.678924</td>\n",
       "      <td>0.166794</td>\n",
       "      <td>0.268718</td>\n",
       "      <td>0.504951</td>\n",
       "      <td>0.833064</td>\n",
       "      <td>0.135552</td>\n",
       "      <td>0.115479</td>\n",
       "      <td>0.385774</td>\n",
       "      <td>0.221927</td>\n",
       "      <td>0.850082</td>\n",
       "      <td>0.099754</td>\n",
       "      <td>0.214464</td>\n",
       "      <td>0.532101</td>\n",
       "      <td>0.699227</td>\n",
       "      <td>0.685787</td>\n",
       "      <td>0.097395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.009858</td>\n",
       "      <td>0.014145</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.016028</td>\n",
       "      <td>0.117458</td>\n",
       "      <td>0.138144</td>\n",
       "      <td>0.107984</td>\n",
       "      <td>0.119156</td>\n",
       "      <td>0.024506</td>\n",
       "      <td>0.056925</td>\n",
       "      <td>0.164126</td>\n",
       "      <td>0.083626</td>\n",
       "      <td>0.056964</td>\n",
       "      <td>0.095137</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.106746</td>\n",
       "      <td>0.198668</td>\n",
       "      <td>0.101556</td>\n",
       "      <td>0.043831</td>\n",
       "      <td>0.087353</td>\n",
       "      <td>0.146113</td>\n",
       "      <td>0.125287</td>\n",
       "      <td>0.095631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.992852</td>\n",
       "      <td>0.966693</td>\n",
       "      <td>0.966017</td>\n",
       "      <td>0.969586</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.614517</td>\n",
       "      <td>0.155123</td>\n",
       "      <td>0.245558</td>\n",
       "      <td>0.391975</td>\n",
       "      <td>0.808148</td>\n",
       "      <td>0.095857</td>\n",
       "      <td>0.046675</td>\n",
       "      <td>0.367774</td>\n",
       "      <td>0.158477</td>\n",
       "      <td>0.721101</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.185082</td>\n",
       "      <td>0.493923</td>\n",
       "      <td>0.673623</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.028487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972620</td>\n",
       "      <td>0.971860</td>\n",
       "      <td>0.978705</td>\n",
       "      <td>0.156556</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.674863</td>\n",
       "      <td>0.167463</td>\n",
       "      <td>0.263026</td>\n",
       "      <td>0.507428</td>\n",
       "      <td>0.825125</td>\n",
       "      <td>0.119465</td>\n",
       "      <td>0.108179</td>\n",
       "      <td>0.394068</td>\n",
       "      <td>0.186442</td>\n",
       "      <td>0.927973</td>\n",
       "      <td>0.067833</td>\n",
       "      <td>0.211584</td>\n",
       "      <td>0.530811</td>\n",
       "      <td>0.714314</td>\n",
       "      <td>0.693302</td>\n",
       "      <td>0.069356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978041</td>\n",
       "      <td>0.977194</td>\n",
       "      <td>0.983789</td>\n",
       "      <td>0.235412</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.741204</td>\n",
       "      <td>0.174619</td>\n",
       "      <td>0.306179</td>\n",
       "      <td>0.583439</td>\n",
       "      <td>0.864632</td>\n",
       "      <td>0.161901</td>\n",
       "      <td>0.151398</td>\n",
       "      <td>0.423006</td>\n",
       "      <td>0.276522</td>\n",
       "      <td>0.965468</td>\n",
       "      <td>0.130968</td>\n",
       "      <td>0.254038</td>\n",
       "      <td>0.571012</td>\n",
       "      <td>0.745377</td>\n",
       "      <td>0.746495</td>\n",
       "      <td>0.139029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shop_review_positive_rate  shop_score_service  shop_score_delivery  \\\n",
       "count              496509.000000       496509.000000        496509.000000   \n",
       "mean                    0.994898            0.971535             0.970906   \n",
       "std                     0.009858            0.014145             0.014084   \n",
       "min                     0.000000            0.000000             0.000000   \n",
       "25%                     0.992852            0.966693             0.966017   \n",
       "50%                     1.000000            0.972620             0.971860   \n",
       "75%                     1.000000            0.978041             0.977194   \n",
       "max                     1.000000            1.000000             1.000000   \n",
       "\n",
       "       shop_score_description  predict_category_property  predict_richness  \\\n",
       "count           496509.000000              496509.000000     496509.000000   \n",
       "mean                 0.975237                   0.164666          0.293092   \n",
       "std                  0.016028                   0.117458          0.138144   \n",
       "min                  0.000000                   0.000000          0.000000   \n",
       "25%                  0.969586                   0.083333          0.153846   \n",
       "50%                  0.978705                   0.156556          0.307692   \n",
       "75%                  0.983789                   0.235412          0.384615   \n",
       "max                  1.000000                   1.000000          1.000000   \n",
       "\n",
       "       item_property_richness  exp_d_user_age_level  exp_d_item_pv_level  \\\n",
       "count           496509.000000         496509.000000        496509.000000   \n",
       "mean                 0.302926              0.678924             0.166794   \n",
       "std                  0.107984              0.119156             0.024506   \n",
       "min                  0.000000              0.000000             0.000000   \n",
       "25%                  0.244681              0.614517             0.155123   \n",
       "50%                  0.287234              0.674863             0.167463   \n",
       "75%                  0.351064              0.741204             0.174619   \n",
       "max                  1.000000              1.000000             1.000000   \n",
       "\n",
       "       exp_d_shop_review_num_level  exp_d_item_sales_level  \\\n",
       "count                496509.000000           496509.000000   \n",
       "mean                      0.268718                0.504951   \n",
       "std                       0.056925                0.164126   \n",
       "min                       0.000000                0.000000   \n",
       "25%                       0.245558                0.391975   \n",
       "50%                       0.263026                0.507428   \n",
       "75%                       0.306179                0.583439   \n",
       "max                       1.000000                1.000000   \n",
       "\n",
       "       exp_d_user_occupation_id  exp_d_item_city_id  exp_d_item_brand_id  \\\n",
       "count             496509.000000       496509.000000        496509.000000   \n",
       "mean                   0.833064            0.135552             0.115479   \n",
       "std                    0.083626            0.056964             0.095137   \n",
       "min                    0.000000            0.000000             0.000000   \n",
       "25%                    0.808148            0.095857             0.046675   \n",
       "50%                    0.825125            0.119465             0.108179   \n",
       "75%                    0.864632            0.161901             0.151398   \n",
       "max                    1.000000            1.000000             1.000000   \n",
       "\n",
       "       exp_d_user_star_level  exp_d_item_price_level      exp_d_day  \\\n",
       "count          496509.000000           496509.000000  496509.000000   \n",
       "mean                0.385774                0.221927       0.850082   \n",
       "std                 0.062493                0.106746       0.198668   \n",
       "min                 0.000000                0.000000       0.000000   \n",
       "25%                 0.367774                0.158477       0.721101   \n",
       "50%                 0.394068                0.186442       0.927973   \n",
       "75%                 0.423006                0.276522       0.965468   \n",
       "max                 1.000000                1.000000       1.000000   \n",
       "\n",
       "       exp_d_item_id  exp_d_shop_star_level  exp_d_item_collected_level  \\\n",
       "count  496509.000000          496509.000000               496509.000000   \n",
       "mean        0.099754               0.214464                    0.532101   \n",
       "std         0.101556               0.043831                    0.087353   \n",
       "min         0.000000               0.000000                    0.000000   \n",
       "25%         0.030928               0.185082                    0.493923   \n",
       "50%         0.067833               0.211584                    0.530811   \n",
       "75%         0.130968               0.254038                    0.571012   \n",
       "max         1.000000               1.000000                    1.000000   \n",
       "\n",
       "       exp_d_user_gender_id  exp_d_context_page_id  exp_d_shop_id  \n",
       "count         496509.000000          496509.000000  496509.000000  \n",
       "mean               0.699227               0.685787       0.097395  \n",
       "std                0.146113               0.125287       0.095631  \n",
       "min                0.000000               0.000000       0.000000  \n",
       "25%                0.673623               0.663871       0.028487  \n",
       "50%                0.714314               0.693302       0.069356  \n",
       "75%                0.745377               0.746495       0.139029  \n",
       "max                1.000000               1.000000       1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1035ee250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#数据的预处理\n",
    "#double系列(例如好评率)的数据存在一个问题，没有考虑到评论量，所以乘了一个评论level，类似 好评率*评论数(分桶了) = 好评量(分桶了)\n",
    "#fillempty把-1设置成-0.01，配合mov2pos使用\n",
    "#mov2pos把有<0的数据都集体-min，如果一个数据是0-1，有-1出现的时候，上面设置成了-0.01，所以这列数据真实是-0.01 - 1,然后我会集体-(-0.01)\n",
    "#norm用了最大值最小值norm\n",
    "%run FeatureProcess.py\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def bucketizer(df, key, bins):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    prefix=\"*BUK*_\"\n",
    "    df[prefix+key] = pd.cut(df[key].values, bins=5, retbins=False)\n",
    "    df[prefix+key] = le.fit_transform(df[prefix+key].copy())\n",
    "    return df\n",
    "\n",
    "\n",
    "def layering(df, key, by_key):\n",
    "    uniqv = df[by_key].unique()\n",
    "    keys = []\n",
    "    for v in uniqv:\n",
    "        indexer = (df[by_key] == v)\n",
    "        new_key = \"*%sLAY*_%s\" % (str(v), key)\n",
    "        df.loc[indexer, new_key] = df[indexer][key]\n",
    "        keys.append(new_key)\n",
    "    return df, keys\n",
    "\n",
    "# #分桶+打散\n",
    "# print(\"bucketizer ... \")\n",
    "# train_df = bucketizer(train_df, \"shop_review_num_level\", 5)\n",
    "\n",
    "# print(\"layering ... \")\n",
    "# new_n = []\n",
    "# for n in numerical:\n",
    "#     train_df, keys = layering(train_df, n, '*BUK*_shop_review_num_level')\n",
    "#     new_n += keys\n",
    "\n",
    "\n",
    "\n",
    "featProc = FeatureProcess(target=target, categorical=categorical, numerical=numerical, listype = listype)\n",
    "# for n in numerical:\n",
    "#     featProc.addLayeringOrgKeys(n)\n",
    "# train_df[\"shop_review_num_level\"] = train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_review_positive_rate\"] = train_df[\"shop_review_positive_rate\"]*train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_score_service\"] = train_df[\"shop_score_service\"]*train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_score_delivery\"] = train_df[\"shop_score_delivery\"]*train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_score_description\"] = train_df[\"shop_score_description\"]*train_df[\"shop_review_num_level\"]\n",
    "\n",
    "train_df = featProc.fillempty(train_df, -0.01)\n",
    "train_df = featProc.mov2pos(train_df)\n",
    "train_df = featProc.norm(train_df)\n",
    "train_df[filter_on_cols.get_raw_numerical_cols()].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toOneHot ...\n",
      "toOneHotList ...\n"
     ]
    }
   ],
   "source": [
    "#onehot没啥好说的，就是onehot，toOneHotList实现有点不同分开了而已\n",
    "\n",
    "\n",
    "train_df = featProc.toOneHot(train_df)\n",
    "train_df = featProc.toOneHotList(train_df)\n",
    "\n",
    "# train_df = featProc.cacheRun(featProc.toOneHot, train_df)\n",
    "\n",
    "for c in train_df.columns:\n",
    "    print(c)\n",
    "\n",
    "print train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ffmyh.py\n",
    "\n",
    "    \n",
    "# def train_and_test_combine(train_df, test_df, res_df):\n",
    "    \n",
    "#     X_train = train_df.copy()\n",
    "#     del X_train['is_trade']\n",
    "#     y_train = train_df['is_trade']\n",
    "    \n",
    "#     X_test = test_df.copy()\n",
    "#     del X_test['is_trade']\n",
    "#     y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "    \n",
    "#     model = xgboost.XGBClassifier(nthread=7,max_depth=5)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     y_prev_predict_train = pd.DataFrame(model.predict_proba(X_train))\n",
    "#     y_prev_predict_train[\"idx\"] = X_train.index\n",
    "#     y_prev_predict_train = y_prev_predict_train.set_index(\"idx\")\n",
    "#     X_train[\"y_prev_predict\"] = y_prev_predict_train[1]\n",
    "    \n",
    "#     y_prev_predict_test = pd.DataFrame(model.predict_proba(X_test))\n",
    "#     y_prev_predict_test[\"idx\"] = X_test.index\n",
    "#     y_prev_predict_test = y_prev_predict_test.set_index(\"idx\")\n",
    "#     X_test[\"y_prev_predict\"] = y_prev_predict_test[1]\n",
    "    \n",
    "    \n",
    "# #     X_train = X_train.fillna(0)\n",
    "# #     X_test = X_test.fillna(0)\n",
    "    \n",
    "    \n",
    "#     model = LogisticRegression(max_iter=1000)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "#     train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "#     test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "#     del train_df\n",
    "#     del test_df\n",
    "    \n",
    "#     del X_train\n",
    "#     del y_train\n",
    "#     del X_test\n",
    "#     del y_test\n",
    "    \n",
    "#     return train_lls, test_lls\n",
    "    \n",
    "\n",
    "def train_and_test_xgboost(train_df, test_df, res_df):    \n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "#     model = xgboost.XGBClassifier(reg_lambda=1.5,learning_rate=0.1,reg_alpha=0,nthread=7,n_estimators=100,max_depth=10)\n",
    "    model = xgboost.XGBClassifier(subsample=0.9,colsample_bytree=0.9,n_estimators=500,max_depth=7,nthread=7)\n",
    "\n",
    "#     model = xgboost.XGBClassifier(subsample=0.9,colsample_bytree=0.9,n_estimators=50,max_depth=3,nthread=7)\n",
    "    model.fit(X_train, y_train)\n",
    "    fig, ax = plt.subplots(figsize=(12,18))\n",
    "    xgboost.plot_importance(model,ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"xgboost_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "def train_and_test_lr(train_df, test_df, res_df):    \n",
    "    X_train = train_df.copy().fillna(0)\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy().fillna(0)\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = LogisticRegression(C=1.0, n_jobs=7,tol=1e-6, max_iter=2000)\n",
    "    model.fit(X_train.values, y_train)\n",
    "#     for idx, val in enumerate(list(X_train.columns)):\n",
    "#         print(\"%s=%s\" %(val,list(model.coef_[0])[idx]))\n",
    "\n",
    "\n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        res_df = res_df.copy().fillna(0)\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"lr_res.csv\",index=False,sep=' ')\n",
    "    \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "def train_and_test_randomforest(train_df, test_df, res_df):    \n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "#     rf = RandomForestClassifier(n_estimators=32, max_depth=40, min_samples_split=100, min_samples_leaf=10,  criterion='entropy',\n",
    "#                         max_features=8, verbose = 1,  bootstrap=False, n_jobs=10)\n",
    "#     RandomForestClassifier(criterion='entropy',n_estimators=100,n_jobs=15)\n",
    "\n",
    "    model = RandomForestClassifier(n_jobs=7, n_estimators=100, max_depth=10, min_samples_split=100, min_samples_leaf=10,  criterion='entropy', max_features=8, verbose = 1,  bootstrap=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    ft_w = pd.DataFrame(model.feature_importances_, columns=[\"weights\"], index = list(X_train.columns))\n",
    "    print ft_w.sort_values(\"weights\", ascending=False)\n",
    "#     fig, ax = plt.subplots(figsize=(12,18))\n",
    "#     xgboost.plot_importance(model,ax=ax)\n",
    "#     plt.show()\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"randomforest_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "def train_and_test_gbdt(train_df, test_df, res_df):\n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=100, max_depth=10, min_samples_split=100, min_samples_leaf=10, max_features=4)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"gbdt_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "\n",
    "def train_and_test_lgb(train_df, test_df, res_df, target, features, categorical):\n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     test['lgb_predict'] = clf.predict_proba(test[features],)[:, 1]\n",
    "#     print(log_loss(test[target], test['lgb_predict']))\n",
    "\n",
    "    \n",
    "    model = lgb.LGBMClassifier(num_leaves=10, max_depth=7, n_estimators=80, n_jobs=20)\n",
    "    model.fit(X_train, y_train, feature_name=features,categorical_feature=categorical)\n",
    "    \n",
    "    ft_w = pd.DataFrame(model.feature_importances_, columns=[\"weights\"], index = list(X_train.columns))\n",
    "    print ft_w\n",
    "#     fig, ax = plt.subplots(figsize=(12,18))\n",
    "#     xgboost.plot_importance(model,ax=ax)\n",
    "#     plt.show()\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"lgb_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "\n",
    "def train_and_test_FFM(train_df, test_df, res_df):\n",
    "    global filter_on_cols\n",
    "    \n",
    "    model = FFM(target=filter_on_cols.target, categorical=filter_on_cols.categorical, numerical=filter_on_cols.numerical, listype=filter_on_cols.listype,\n",
    "                                        reg_param = 0.0004,\n",
    "                                        k = 4,\n",
    "                                        iter_max = 500,\n",
    "                                        learing_rate = 0.1,\n",
    "                                        threads = 7,\n",
    "                                        auto_stop = False,\n",
    "                                        quiet = False,\n",
    "                                        no_norm = False)\n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(train_df, test_df)\n",
    "    \n",
    "    train_lls = log_loss(train_df[\"is_trade\"],model.predict_proba(train_df))\n",
    "    test_lls = log_loss(test_df[\"is_trade\"],model.predict_proba(test_df))\n",
    "    \n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"ffm.csv\",index=False,sep=' ')\n",
    "        \n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "\n",
    "def select_best_features(df, precent=0.8):\n",
    "    from sklearn.feature_selection import SelectKBest,chi2 \n",
    "    df = df.copy()\n",
    "    \n",
    "    for u in df.columns:\n",
    "        df[df[u] == -1] = 0\n",
    "\n",
    "    X_train = df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = df['is_trade']\n",
    "    \n",
    "    c = int(len(X_train.columns) * precent)\n",
    "    skb = SelectKBest(chi2,k=c)\n",
    "    skb.fit_transform(X_train,y_train)\n",
    "    \n",
    "    old_fea = list(X_train.columns)\n",
    "    new_fea = []\n",
    "    for idx, b in enumerate(list(skb.get_support())):\n",
    "        if b:\n",
    "            new_fea.append(old_fea[idx])\n",
    "        else:\n",
    "            print(\"Filter feature:%s\" % old_fea[idx])\n",
    "    return new_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开搞！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org dataset = 496509\n",
      "0    78268\n",
      "3    71199\n",
      "1    70931\n",
      "2    68387\n",
      "4    68318\n",
      "5    63614\n",
      "6    57421\n",
      "7    18371\n",
      "Name: day, dtype: int64\n",
      "0    78268\n",
      "3    71199\n",
      "1    70931\n",
      "2    68387\n",
      "4    68318\n",
      "5    63614\n",
      "6    57421\n",
      "7    18371\n",
      "Name: day, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:   46.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           weights\n",
      "item_price_level                          0.169938\n",
      "shop_score_delivery                       0.108006\n",
      "item_sales_level                          0.086737\n",
      "shop_score_service                        0.079055\n",
      "hour                                      0.068266\n",
      "shop_score_description                    0.066195\n",
      "item_city_id                              0.043262\n",
      "user_age_level                            0.040727\n",
      "shop_review_positive_rate                 0.040258\n",
      "user_star_level                           0.039385\n",
      "item_id                                   0.038696\n",
      "shop_id                                   0.032035\n",
      "item_brand_id                             0.030860\n",
      "item_collected_level                      0.026127\n",
      "context_page_id                           0.025816\n",
      "shop_star_level                           0.019870\n",
      "item_pv_level                             0.019365\n",
      "shop_review_num_level                     0.019301\n",
      "user_gender_id*shop_review_positive_rate  0.016855\n",
      "weekday                                   0.013308\n",
      "user_occupation_id                        0.010076\n",
      "user_gender_id                            0.005864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "/Users/yuhua/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 23 dimension TrainLogLoss = 0.0852914533988 | TestLogLoss = 0.0825883799059\n",
      "Filter feature:user_gender_id\n",
      "Filter feature:user_gender_id*shop_review_positive_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:   48.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            weights\n",
      "item_price_level           0.164341\n",
      "shop_score_delivery        0.106576\n",
      "item_sales_level           0.093089\n",
      "shop_score_service         0.082014\n",
      "hour                       0.070538\n",
      "shop_score_description     0.068682\n",
      "shop_review_positive_rate  0.044621\n",
      "user_age_level             0.041776\n",
      "user_star_level            0.040521\n",
      "item_id                    0.040341\n",
      "item_city_id               0.039873\n",
      "shop_id                    0.034254\n",
      "item_brand_id              0.031838\n",
      "context_page_id            0.029319\n",
      "item_collected_level       0.026517\n",
      "shop_review_num_level      0.019948\n",
      "shop_star_level            0.019791\n",
      "item_pv_level              0.019522\n",
      "weekday                    0.015329\n",
      "user_occupation_id         0.011108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 21 dimension TrainLogLoss = 0.0852287363821 | TestLogLoss = 0.0825099503899\n"
     ]
    }
   ],
   "source": [
    "#这块一般不怎么需要改了，拿最后一天做验证\n",
    "#注意下面验证版和测试版，测试版是我们自己玩的，验证版是扔线上的，通过注释不同的跑就行了\n",
    "\n",
    "\n",
    "# %run BaseFrame.py\n",
    "% run ../util/time_series_split.py\n",
    "\n",
    "tmp_train_df = train_df.copy().fillna(0)\n",
    "\n",
    "tmp_train_df[\"user_gender_id*shop_review_positive_rate\"] = tmp_train_df[\"user_gender_id\"] * (1+tmp_train_df[\"shop_review_positive_rate\"])\n",
    "\n",
    "# tmp_train_df[\"user_gender_id*shop_star_level\"] = tmp_train_df[\"user_gender_id\"] * (1+tmp_train_df[\"shop_star_level\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categorical_one_hot_cols = filter_on_cols.get_onehoted_cols(\"categorical\", tmp_train_df)\n",
    "listype_one_hot_cols = filter_on_cols.get_onehoted_cols(\"listype\", tmp_train_df)\n",
    "\n",
    "index_col = \"instance_id\"\n",
    "\n",
    "\n",
    "categorical_cols = filter_on_cols.get_raw_categorical_cols()\n",
    "listype_cols = filter_on_cols.get_raw_listype_cols()\n",
    "numerical_cols = filter_on_cols.get_raw_numerical_cols()+[\"user_gender_id*shop_review_positive_rate\"]\n",
    "target_col = filter_on_cols.get_raw_target_col()\n",
    "\n",
    "\n",
    "print(\"org dataset = %s\" % len(tmp_train_df))\n",
    "print(tmp_train_df[\"day\"].value_counts())\n",
    "\n",
    "\n",
    "# tmp_train_df = featProc.balance_pos_neg_sample(tmp_train_df, (1,10))\n",
    "# print \"sampling dataset =\", len(tmp_train_df)\n",
    "# print tmp_train_df[\"day\"].value_counts()\n",
    "\n",
    "# 抽样\n",
    "# tmp_train_df = tmp_train_df.sample(n=10000, random_state=666)\n",
    "# print \"sampling dataset =\", len(tmp_train_df)\n",
    "\n",
    "print(tmp_train_df[\"day\"].value_counts())\n",
    "\n",
    "#测试版\n",
    "valid_df = tmp_train_df.loc[tmp_train_df[\"day\"]==6]\n",
    "tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]<6]\n",
    "# tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]>0]\n",
    "res_df=valid_df.copy()\n",
    "\n",
    "# #线上验证版\n",
    "# valid_df = tmp_train_df.loc[tmp_train_df[\"day\"]==6]\n",
    "# res_df = tmp_train_df.loc[tmp_train_df[\"day\"]==7]\n",
    "# tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]<=6]\n",
    "\n",
    "# print(\"res dataset = %s\" % len(res_df))\n",
    "# print(\"valid dateset = %s\" % len(valid_df))\n",
    "# print(\"train dateset = %s\" % len(tmp_train_df))\n",
    "\n",
    "# tmp_train_df = tmp_train_df[on_cols]\n",
    "# valid_df = valid_df[on_cols]\n",
    "\n",
    "\n",
    "# on_cols = [target_col]+numerical_cols+categorical_one_hot_cols+listype_one_hot_cols\n",
    "# # on_cols = [target_col]+numerical_cols\n",
    "# # +categorical_one_hot_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_lr(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"LR %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "\n",
    "on_cols = [target_col]+categorical_cols+numerical_cols\n",
    "# on_cols = [target_col]+select_best_features(tmp_train_df[on_cols], 0.8)\n",
    "train_lls, test_lls = train_and_test_randomforest(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "print(\"XGBoost %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "\n",
    "on_cols = [target_col]+categorical_cols+numerical_cols\n",
    "on_cols = [target_col]+select_best_features(tmp_train_df[on_cols], 0.95)\n",
    "train_lls, test_lls = train_and_test_randomforest(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "print(\"XGBoost %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# +listype_one_hot_cols\n",
    "# on_cols = ffea(on_cols)\n",
    "\n",
    "\n",
    "\n",
    "# on_cols = [target_col]+categorical_cols+numerical_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_randomforest(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"RandomForest %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "# on_cols = [target_col]+categorical_cols+numerical_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_gbdt(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"GBDT %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "# train_lls, test_lls = train_and_test_combine(tmp_train_df[on_cols], valid_df[on_cols])\n",
    "# print \"Combine TrainLogLoss = %s | TestLogLoss = %s\" % (train_lls, test_lls)\n",
    "\n",
    "# on_cols = [target_col]+numerical_cols+categorical_one_hot_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_FFM(tmp_train_df, valid_df)\n",
    "# print \"FFM TrainLogLoss = %s | TestLogLoss = %s\" % (train_lls, test_lls)\n",
    "\n",
    "# train_lls, test_lls = train_and_test_FFM(tmp_train_df, valid_df, res_df)\n",
    "# print(\"LR %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_valid_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-02e311444a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_valid_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# valid_df = gen_valid_set_FULL(tmp_train_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtmp_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"day\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_valid_set' is not defined"
     ]
    }
   ],
   "source": [
    "tmp_train_df = train_df.copy()\n",
    "valid_df = gen_valid_set(tmp_train_df)\n",
    "# valid_df = gen_valid_set_FULL(tmp_train_df)\n",
    "\n",
    "tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]<6]\n",
    "res_df=valid_df.copy()\n",
    "\n",
    "\n",
    "print len(tmp_train_df),  len(valid_df)\n",
    "\n",
    "\n",
    "categorical_one_hot_cols = filter_on_cols.get_onehoted_cols(\"categorical\", tmp_train_df)\n",
    "listype_one_hot_cols = filter_on_cols.get_onehoted_cols(\"listype\", tmp_train_df)\n",
    "\n",
    "index_col = \"instance_id\"\n",
    "\n",
    "categorical_cols = filter_on_cols.get_raw_categorical_cols()\n",
    "listype_cols = filter_on_cols.get_raw_listype_cols()\n",
    "numerical_cols = filter_on_cols.get_raw_numerical_cols()\n",
    "target_col = filter_on_cols.get_raw_target_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(valid_df)\n",
    "print valid_df[\"is_trade\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "on_cols = [target_col]+numerical_cols+categorical_one_hot_cols#+listype_one_hot_cols\n",
    "# print on_cols\n",
    "train_lls, test_lls = train_and_test_lr(tmp_train_df[on_cols], valid_df[on_cols], None)\n",
    "print(\"LR TrainLogLoss = %s | TestLogLoss = %s\" % (train_lls, test_lls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = valid_df.copy()\n",
    "del X_test['is_trade']\n",
    "y_test = valid_df.copy()['is_trade']\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in xrange(1000000):\n",
    "    X_val_a, X_val_b, y_val_a, y_val_b = train_test_split(X_test, y_test, test_size=0.7, shuffle=True, random_state=i)\n",
    "    zero = []\n",
    "    for j in xrange(len(y_val_a)):\n",
    "        f = float('0.0000000000000%s' % random.randint(1,99))\n",
    "        zero.append(f)\n",
    "    lls = log_loss(y_val_a, zero)\n",
    "    llsdiff = abs(0.31284 - lls)\n",
    "    y_val_a = pd.DataFrame(y_val_a)\n",
    "    l = float(len(y_val_a))\n",
    "    \n",
    "    if llsdiff < 0.0001:\n",
    "        print i, \"[HIT]\", llsdiff, lls, len(y_val_a[y_val_a[\"is_trade\"] == 0])/l, len(y_val_a[y_val_a[\"is_trade\"] == 1])/l\n",
    "    else:\n",
    "        print i, llsdiff, lls, len(y_val_a[y_val_a[\"is_trade\"] == 0])/l, len(y_val_a[y_val_a[\"is_trade\"] == 1])/l\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = pd.read_table('/Users/yuhua/res1.csv',sep=' ')\n",
    "log_loss(a, t[\"predicted_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print filter_on_cols.numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ffmyh.py\n",
    "%run FeatureProcess.py\n",
    "\n",
    "fpfp = FeatureProcess(target=target, categorical=categorical, numerical=numerical, listype = listype)\n",
    "# for n in numerical:\n",
    "#     fpfp.addLayeringOrgKeys(n)\n",
    "#一定要fit一个全集\n",
    "fpfp.fit(train_df)\n",
    "\n",
    "fpfp.toFFMData(tmp_train_df, \"one_hot-train.txt\")\n",
    "fpfp.toFFMData(valid_df, \"one_hot-valid.txt\")\n",
    "# model = FFM(fpfp,\n",
    "#             reg_param = 0.00001,\n",
    "#             k = 4,\n",
    "#             iter_max = 500,\n",
    "#             learing_rate = 0.01,\n",
    "#             threads = 7,\n",
    "#             auto_stop = False,\n",
    "#             quiet = False)\n",
    "\n",
    "\n",
    "    \n",
    "# model.fit(tmp_train_df, valid_df)\n",
    "\n",
    "# train_lls = log_loss(train_df[\"is_trade\"],model.predict_proba(train_df))\n",
    "# test_lls = log_loss(test_df[\"is_trade\"],model.predict_proba(test_df))\n",
    "\n",
    "# print train_lls, test_lls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- | XGboost | 增减 | LR |增减\n",
    "- | :-: | :-: | :-: | :-:\n",
    "原始特征 | 0.0829350458274 | / | 0.0829428179401 | /\n",
    "原始特征+时间 | 0.0828004994761 | +++ | 0.082869098617 | +\n",
    "同上+按天计算交易率 | 0.0828218434014 | - | 0.0827914564667 | ++\n",
    "同上+predict_prop_cate相似度 | 0.082778758021 | ++ | 0.082798919764 | --\n",
    "同上+cate prop丰富度 | 0.0828356414143 | -- | 0.0828009736655 | -\n",
    "同上+cate/prop list OneHot | 0.0828356414143 | / | 0.0827979171146 | +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tmp_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SelectKBest(chi2,k=2).fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
