{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPH第二版实验\n",
    "\n",
    "### 思路\n",
    "\n",
    "*id统计相关*\n",
    "1. item_id, shop_id的交易量, 交易率\n",
    "2. item_id, shop_id7天是否新出现, 3天是否新出现, 昨天是否新出现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "offline_df = pd.read_table('../../round1_ijcai_18_train_20180301.txt', sep=' ')\n",
    "# online_df = pd.read_table('../../round1_ijcai_18_test_a_20180301.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据集统一处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时间处理: 分离天, 星期几, 上中下午/晚上, 小时数\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "def extract_weekday(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.weekday()\n",
    "def extract_hour(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.hour\n",
    "\n",
    "offline_df['date'] = offline_df['context_timestamp'].apply(lambda x: extract_date(x))\n",
    "offline_df['weekday'] = offline_df['context_timestamp'].apply(lambda x: extract_weekday(x))\n",
    "offline_df['hour'] = offline_df['context_timestamp'].apply(lambda x: extract_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../util/time_utils.py\n",
    "\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 3)\n",
    "\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计信息附加\n",
    "\n",
    "def getColTradeRate(df, idCol):\n",
    "    rateCol = idCol + '_tr'\n",
    "    pvCol = idCol + '_pv'\n",
    "    try:\n",
    "        del df[rateCol]\n",
    "        del df[pvCol]\n",
    "    except:\n",
    "        pass\n",
    "    a = offline_df.groupby([idCol]).agg({'is_trade':'sum'})\n",
    "    b = offline_df.groupby([idCol]).agg({'is_trade':'size'})\n",
    "    c = a.join(b, lsuffix=\"_sum\", rsuffix=\"_size\")\n",
    "    c[rateCol] = c['is_trade_sum'] / c['is_trade_size']\n",
    "    c[pvCol] = c['is_trade_size']\n",
    "    return offline_df.join(c[[rateCol, pvCol]], on=idCol)\n",
    "    \n",
    "\n",
    "# 各类id的 交易量, 交易率\n",
    "offline_df = getColTradeRate(offline_df, 'item_city_id')\n",
    "offline_df = getColTradeRate(offline_df, 'item_id')\n",
    "offline_df = getColTradeRate(offline_df, 'shop_id')\n",
    "offline_df = getColTradeRate(offline_df, 'item_brand_id')\n",
    "offline_df = getColTradeRate(offline_df, 'user_occupation_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分桶\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "offline_df['item_city_id_pv_cut'] = pd.cut(offline_df['item_city_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['item_id_pv_cut'] = pd.cut(offline_df['item_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['shop_id_pv_cut'] = pd.cut(offline_df['shop_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['item_brand_id_pv_cut'] = pd.cut(offline_df['item_brand_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['user_occupation_id_pv_cut'] = pd.cut(offline_df['user_occupation_id_pv'].values, bins=10, retbins=False)\n",
    "\n",
    "offline_df['item_city_id_pv_cut'] = le.fit_transform(offline_df['item_city_id_pv_cut'].copy())\n",
    "offline_df['item_id_pv_cut'] = le.fit_transform(offline_df['item_id_pv_cut'].copy())\n",
    "offline_df['shop_id_pv_cut'] = le.fit_transform(offline_df['shop_id_pv_cut'].copy())\n",
    "offline_df['item_brand_id_pv_cut'] = le.fit_transform(offline_df['item_brand_id_pv_cut'].copy())\n",
    "offline_df['user_occupation_id_pv_cut'] = le.fit_transform(offline_df['user_occupation_id_pv_cut'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardization and scaling\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def scale(df, idCol):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df[idCol].values.reshape(-1, 1))\n",
    "    return pd.Series(a.reshape(1, -1)[0])\n",
    "\n",
    "# 令数据再少一些?\n",
    "offline_df['shop_score_service_scaled'] = scale(offline_df, 'shop_score_service')\n",
    "offline_df['shop_score_delivery_scaled'] = scale(offline_df, 'shop_score_delivery')\n",
    "offline_df['shop_score_description_scaled'] = scale(offline_df, 'shop_score_description')\n",
    "offline_df['shop_review_positive_rate_scaled'] = scale(offline_df, 'shop_review_positive_rate')\n",
    "offline_df['item_city_id_tr_scaled'] = scale(offline_df, 'item_city_id_tr')\n",
    "offline_df['item_id_tr_scaled'] = scale(offline_df, 'item_id_tr')\n",
    "offline_df['shop_id_tr_scaled'] = scale(offline_df, 'shop_id_tr')\n",
    "offline_df['item_brand_id_tr_scaled'] = scale(offline_df, 'item_brand_id_tr')\n",
    "offline_df['user_occupation_id_scaled'] = scale(offline_df, 'user_occupation_id_tr')\n",
    "\n",
    "# offline_df = offline_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 生成矩阵数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "used = 78261\n",
    "D = offline_df['context_timestamp']\n",
    "X = offline_df[\n",
    "    [\n",
    "        'item_id', 'item_brand_id', 'item_city_id', 'user_id', 'context_page_id', 'context_id', 'shop_id', \n",
    "        'item_id_dup_g_7', 'shop_id_dup_g_7', 'item_brand_id_dup_g_7', 'user_id_dup_g_7',\n",
    "#         'item_id_dup_g_1', 'shop_id_dup_g_1', 'item_brand_id_dup_g_1', 'user_id_dup_g_1',\n",
    "#         'item_id_dup_g_3', 'shop_id_dup_g_3', 'item_brand_id_dup_g_3', 'user_id_dup_g_3',\n",
    "        'item_id_pv_cut', 'shop_id_pv_cut', 'user_occupation_id_pv_cut', \n",
    "         'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', \n",
    "        'user_gender_id', 'weekday', 'hour', 'user_age_level', 'user_star_level', \n",
    "       'user_occupation_id', \n",
    "       'item_city_id_tr', 'item_city_id_pv_cut', 'item_id_tr', 'item_id_pv',\n",
    "       'shop_id_tr', 'shop_id_pv', 'item_brand_id_tr', 'item_brand_id_pv',\n",
    "       'user_occupation_id_pv', 'shop_score_delivery',\n",
    "        'shop_score_description', 'shop_review_positive_rate', 'shop_score_service'      \n",
    "    ]\n",
    "].values\n",
    "#        'shop_score_delivery_scaled', 'shop_score_description_scaled',\n",
    "#        'shop_review_positive_rate_scaled', 'item_city_id_tr_scaled',\n",
    "#        'item_id_tr_scaled', 'shop_id_tr_scaled', 'item_brand_id_tr_scaled',k\n",
    "#        'user_occupation_id_scaled', 'shop_score_service_scaled', \n",
    "        \n",
    "y = offline_df[['is_trade']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 GridSearchCV 超参搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<__main__.DateTimeSplit object at 0x1a20fa4860>,\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'nthread': [4], 'learning_rate': [0.05], 'max_depth': [6], 'n_estimators': [5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV as GSCV   #Perforing grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "\n",
    "% run ../util/time_series_split.py\n",
    "\n",
    "\n",
    "dtsv = DateTimeSplit(dateSeries=D, fmt=\"%Y-%m-%d\")\n",
    "score_func = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# for train_i, test_i in dtsv.split(X, y):\n",
    "#     X_train, y_train = X[train_i], y[test_i]\n",
    "#     X_test, y_test = X[test_i], y[test_i]\n",
    "    \n",
    "#     print(X_train.shape, y_train.shape)\n",
    "#     print(X_test.shape, y_test.shape)\n",
    "\n",
    "parameters = {\n",
    "  'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "  'learning_rate': [0.05], #so called `eta` value\n",
    "  'max_depth': [6],\n",
    "  'n_estimators': [5]\n",
    "}\n",
    "\n",
    "xgb = xgboost.XGBClassifier()\n",
    "gs = GSCV(xgb, parameters, scoring=score_func, cv=dtsv)\n",
    "gs.fit(X, y)\n",
    "\n",
    "    \n",
    "# best_parameters, score, _ = max(gs.cv_results_, key=lambda x: x[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 1.30268296]), 'std_fit_time': array([ 0.4735773]), 'mean_score_time': array([ 0.04144303]), 'std_score_time': array([ 0.00406109]), 'param_learning_rate': masked_array(data = [0.05],\n",
      "             mask = [False],\n",
      "       fill_value = ?)\n",
      ", 'param_max_depth': masked_array(data = [6],\n",
      "             mask = [False],\n",
      "       fill_value = ?)\n",
      ", 'param_n_estimators': masked_array(data = [5],\n",
      "             mask = [False],\n",
      "       fill_value = ?)\n",
      ", 'param_nthread': masked_array(data = [4],\n",
      "             mask = [False],\n",
      "       fill_value = ?)\n",
      ", 'params': [{'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 5, 'nthread': 4}], 'split0_test_score': array([-0.50556711]), 'split1_test_score': array([-0.50549813]), 'split2_test_score': array([-0.50536553]), 'split3_test_score': array([-0.50496089]), 'split4_test_score': array([-0.50418305]), 'split5_test_score': array([-0.50382902]), 'mean_test_score': array([-0.50494607]), 'std_test_score': array([ 0.00065093]), 'rank_test_score': array([1], dtype=int32), 'split0_train_score': array([-0.50554912]), 'split1_train_score': array([-0.50550399]), 'split2_train_score': array([-0.50540125]), 'split3_train_score': array([-0.50534033]), 'split4_train_score': array([-0.50522245]), 'split5_train_score': array([-0.50496803]), 'mean_train_score': array([-0.50533086]), 'std_train_score': array([ 0.0001941])}\n"
     ]
    }
   ],
   "source": [
    "# best_parameters, score = max(gs.cv_results_, key=lambda x: x[1])\n",
    "print(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 kfold 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = offline_df[\n",
    "    [\n",
    "        'date',\n",
    "        'item_id', 'item_brand_id', 'item_city_id', 'user_id', 'context_page_id', 'context_id', 'shop_id', \n",
    "        'item_id_dup_g_7', 'shop_id_dup_g_7', 'item_brand_id_dup_g_7', 'user_id_dup_g_7',\n",
    "#         'item_id_dup_g_1', 'shop_id_dup_g_1', 'item_brand_id_dup_g_1', 'user_id_dup_g_1',\n",
    "#         'item_id_dup_g_3', 'shop_id_dup_g_3', 'item_brand_id_dup_g_3', 'user_id_dup_g_3',\n",
    "        'item_id_pv_cut', 'shop_id_pv_cut', 'user_occupation_id_pv_cut', \n",
    "         'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', \n",
    "        'user_gender_id', 'weekday', 'hour', 'user_age_level', 'user_star_level', \n",
    "       'user_occupation_id', \n",
    "       'item_city_id_tr', 'item_city_id_pv_cut', 'item_id_tr', 'item_id_pv',\n",
    "       'shop_id_tr', 'shop_id_pv', 'item_brand_id_tr', 'item_brand_id_pv',\n",
    "       'user_occupation_id_pv', 'shop_score_delivery',\n",
    "        'shop_score_description', 'shop_review_positive_rate', 'shop_score_service'      \n",
    "    ]\n",
    "]\n",
    "#        'shop_score_delivery_scaled', 'shop_score_description_scaled',\n",
    "#        'shop_review_positive_rate_scaled', 'item_city_id_tr_scaled',\n",
    "#        'item_id_tr_scaled', 'shop_id_tr_scaled', 'item_brand_id_tr_scaled',k\n",
    "#        'user_occupation_id_scaled', 'shop_score_service_scaled', \n",
    "        \n",
    "y = offline_df[['is_trade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-18 to 2018-09-18 is training set, 2018-09-19 is test set, 2018-09-24 is valid set, start training ... \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'is_trade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_trade'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-22e32fccfdaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_by_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtatx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# 0.077189, 0.076092\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# test = 0.077047  valid = 0.07597\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/SecretTianchiProject/src/yuhua/BaseFrame.py\u001b[0m in \u001b[0;36mkfold_by_date\u001b[0;34m(self, dateCol, callback)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdateCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtrain_ll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             print('%s done, logloss train = %s (compare to previous: %s), \\n\\\n",
      "\u001b[0;32m<ipython-input-24-22e32fccfdaf>\u001b[0m in \u001b[0;36mtatx\u001b[0;34m(train_df, test_df, valid_df)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_trade'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_trade'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1899\u001b[0m             \u001b[0;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;31m# exception:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1901\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m         \u001b[0;31m# delete from the caches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3647\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3648\u001b[0m         \"\"\"\n\u001b[0;32m-> 3649\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3651\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_trade'"
     ]
    }
   ],
   "source": [
    "train_input = X\n",
    "# train_input['is_trade'] = offline_df['is_trade']\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "%run ../yuhua/BaseFrame.py\n",
    "\n",
    "\n",
    "it = 0\n",
    "def tatx(train_df, test_df, valid_df):\n",
    "\n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    X_valid = valid_df.copy()\n",
    "    del X_valid['is_trade']\n",
    "    y_valid = valid_df['is_trade']\n",
    "\n",
    "       \n",
    "    params = {\n",
    "        'nthread': 20\n",
    "    }\n",
    "    m = xgboost.XGBClassifier(**params)\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    l_train = log_loss(y_train, m.predict_proba(X_train))\n",
    "    l_test = log_loss(y_test, m.predict_proba(X_test))\n",
    "    l_valid = log_loss(y_valid, m.predict_proba(X_valid))\n",
    "\n",
    "    return l_train, l_test, l_valid\n",
    "\n",
    "frame = BaseFrame(train_input, 0.05)\n",
    "frame.kfold_by_date('date', tatx)\n",
    "# 0.077189, 0.076092\n",
    "# test = 0.077047  valid = 0.07597\n",
    "# test = 0.077077  valid = 0.075944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
