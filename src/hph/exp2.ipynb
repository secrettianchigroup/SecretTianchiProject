{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPH第二版实验\n",
    "\n",
    "### 树的叶子节点作为特征\n",
    "\n",
    "参考: https://github.com/lytforgood/MachineLearningTrick/blob/master/xgboost%E8%B0%83%E5%8F%82%E6%BC%94%E7%A4%BA.md\n",
    "\n",
    "*改造cv函数, 把按天分割数据封装成DateTimeSplit类*\n",
    "\n",
    "*GridSearchCV调整参数*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "offline_df = pd.read_table('../../round1_ijcai_18_train_20180301.txt', sep=' ')\n",
    "# online_df = pd.read_table('../../round1_ijcai_18_test_a_20180301.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据集统一处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时间处理: 分离天, 星期几, 上中下午/晚上, 小时数\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "def extract_weekday(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.weekday()\n",
    "def extract_hour(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.hour\n",
    "\n",
    "offline_df['date'] = offline_df['context_timestamp'].apply(lambda x: extract_date(x))\n",
    "offline_df['weekday'] = offline_df['context_timestamp'].apply(lambda x: extract_weekday(x))\n",
    "offline_df['hour'] = offline_df['context_timestamp'].apply(lambda x: extract_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../util/time_utils.py\n",
    "\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 3)\n",
    "\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计信息附加\n",
    "\n",
    "def getColTradeRate(df, idCol):\n",
    "    rateCol = idCol + '_tr'\n",
    "    pvCol = idCol + '_pv'\n",
    "    try:\n",
    "        del df[rateCol]\n",
    "        del df[pvCol]\n",
    "    except:\n",
    "        pass\n",
    "    a = offline_df.groupby([idCol]).agg({'is_trade':'sum'})\n",
    "    b = offline_df.groupby([idCol]).agg({'is_trade':'size'})\n",
    "    c = a.join(b, lsuffix=\"_sum\", rsuffix=\"_size\")\n",
    "    c[rateCol] = c['is_trade_sum'] / c['is_trade_size']\n",
    "    c[pvCol] = c['is_trade_size']\n",
    "    return offline_df.join(c[[rateCol, pvCol]], on=idCol)\n",
    "    \n",
    "\n",
    "# 各类id的 交易量, 交易率\n",
    "offline_df = getColTradeRate(offline_df, 'item_city_id')\n",
    "offline_df = getColTradeRate(offline_df, 'item_id')\n",
    "offline_df = getColTradeRate(offline_df, 'shop_id')\n",
    "offline_df = getColTradeRate(offline_df, 'item_brand_id')\n",
    "offline_df = getColTradeRate(offline_df, 'user_occupation_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分桶\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "offline_df['item_city_id_pv_cut'] = pd.cut(offline_df['item_city_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['item_id_pv_cut'] = pd.cut(offline_df['item_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['shop_id_pv_cut'] = pd.cut(offline_df['shop_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['item_brand_id_pv_cut'] = pd.cut(offline_df['item_brand_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['user_occupation_id_pv_cut'] = pd.cut(offline_df['user_occupation_id_pv'].values, bins=10, retbins=False)\n",
    "\n",
    "offline_df['item_city_id_pv_cut'] = le.fit_transform(offline_df['item_city_id_pv_cut'].copy())\n",
    "offline_df['item_id_pv_cut'] = le.fit_transform(offline_df['item_id_pv_cut'].copy())\n",
    "offline_df['shop_id_pv_cut'] = le.fit_transform(offline_df['shop_id_pv_cut'].copy())\n",
    "offline_df['item_brand_id_pv_cut'] = le.fit_transform(offline_df['item_brand_id_pv_cut'].copy())\n",
    "offline_df['user_occupation_id_pv_cut'] = le.fit_transform(offline_df['user_occupation_id_pv_cut'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardization and scaling\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def scale(df, idCol):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df[idCol].values.reshape(-1, 1))\n",
    "    return pd.Series(a.reshape(1, -1)[0])\n",
    "\n",
    "# 令数据再少一些?\n",
    "offline_df['shop_score_service_scaled'] = scale(offline_df, 'shop_score_service')\n",
    "offline_df['shop_score_delivery_scaled'] = scale(offline_df, 'shop_score_delivery')\n",
    "offline_df['shop_score_description_scaled'] = scale(offline_df, 'shop_score_description')\n",
    "offline_df['shop_review_positive_rate_scaled'] = scale(offline_df, 'shop_review_positive_rate')\n",
    "offline_df['item_city_id_tr_scaled'] = scale(offline_df, 'item_city_id_tr')\n",
    "offline_df['item_id_tr_scaled'] = scale(offline_df, 'item_id_tr')\n",
    "offline_df['shop_id_tr_scaled'] = scale(offline_df, 'shop_id_tr')\n",
    "offline_df['item_brand_id_tr_scaled'] = scale(offline_df, 'item_brand_id_tr')\n",
    "offline_df['user_occupation_id_scaled'] = scale(offline_df, 'user_occupation_id_tr')\n",
    "\n",
    "# offline_df = offline_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 生成矩阵数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "used = 78261\n",
    "D = offline_df['context_timestamp']\n",
    "X = offline_df[\n",
    "    [\n",
    "        'item_id', 'item_brand_id', 'item_city_id', 'user_id', 'context_page_id', 'context_id', 'shop_id', \n",
    "        'item_id_dup_g_7', 'shop_id_dup_g_7', 'item_brand_id_dup_g_7', 'user_id_dup_g_7',\n",
    "#         'item_id_dup_g_1', 'shop_id_dup_g_1', 'item_brand_id_dup_g_1', 'user_id_dup_g_1',\n",
    "#         'item_id_dup_g_3', 'shop_id_dup_g_3', 'item_brand_id_dup_g_3', 'user_id_dup_g_3',\n",
    "        'item_id_pv_cut', 'shop_id_pv_cut', 'user_occupation_id_pv_cut', \n",
    "         'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', \n",
    "        'user_gender_id', 'weekday', 'hour', 'user_age_level', 'user_star_level', \n",
    "       'user_occupation_id', \n",
    "       'item_city_id_tr', 'item_city_id_pv_cut', 'item_id_tr', 'item_id_pv',\n",
    "       'shop_id_tr', 'shop_id_pv', 'item_brand_id_tr', 'item_brand_id_pv',\n",
    "       'user_occupation_id_pv', 'shop_score_delivery',\n",
    "        'shop_score_description', 'shop_review_positive_rate', 'shop_score_service'      \n",
    "    ]\n",
    "].values\n",
    "#        'shop_score_delivery_scaled', 'shop_score_description_scaled',\n",
    "#        'shop_review_positive_rate_scaled', 'item_city_id_tr_scaled',\n",
    "#        'item_id_tr_scaled', 'shop_id_tr_scaled', 'item_brand_id_tr_scaled',k\n",
    "#        'user_occupation_id_scaled', 'shop_score_service_scaled', \n",
    "        \n",
    "y = offline_df[['is_trade']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 GridSearchCV 超参搜索框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV as GSCV   #Perforing grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "\n",
    "%run ../util/time_series_split.py\n",
    "\n",
    "\n",
    "dtsv = DateTimeSplit(dateSeries=D, fmt=\"%Y-%m-%d\")\n",
    "dtsv2 = DateTimeSplit2(dateCol='day', gap=6)\n",
    "\n",
    "score_func = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "\"\"\"\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       n_jobs=1, nthread=20, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    'nthread':[20], #when use hyperthread, xgboost may become slower\n",
    "    'learning_rate': [0.1], #so called `eta` value\n",
    "    'colsample_bylevel': [1],\n",
    "    'max_delta_step':[0],\n",
    "    'colsample_bytree':[1],\n",
    "    'gamma':[0],\n",
    "    'max_depth': [3],\n",
    "    'missing':[None],\n",
    "    'min_child_weight':[1],\n",
    "    'scale_pos_weight':[1],\n",
    "    'n_estimators': [100],\n",
    "    'base_score':[0.5],\n",
    "    'booster':['gbtree'],\n",
    "    'objective':['binary:logistic'],\n",
    "    'reg_alpha':[0],\n",
    "    'reg_lambda':[1],\n",
    "    'silent':[True],\n",
    "    'subsample':[1]\n",
    "}\n",
    "\n",
    "xgb = xgboost.XGBClassifier()\n",
    "gs = GSCV(xgb, parameters, scoring=score_func, cv=dtsv)\n",
    "# gs.fit(X, y)\n",
    "\n",
    "print(gs.best_estimator_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 按天时间序列kfold 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = offline_df['context_timestamp']\n",
    "X = offline_df[\n",
    "    [\n",
    "        'item_id', 'item_brand_id', 'item_city_id', 'user_id', 'context_page_id', 'context_id', 'shop_id', \n",
    "        'item_id_dup_g_7', 'shop_id_dup_g_7', 'item_brand_id_dup_g_7', 'user_id_dup_g_7',\n",
    "#         'item_id_dup_g_1', 'shop_id_dup_g_1', 'item_brand_id_dup_g_1', 'user_id_dup_g_1',\n",
    "#         'item_id_dup_g_3', 'shop_id_dup_g_3', 'item_brand_id_dup_g_3', 'user_id_dup_g_3',\n",
    "        'item_id_pv_cut', 'shop_id_pv_cut', 'user_occupation_id_pv_cut', \n",
    "         'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', \n",
    "        'user_gender_id', 'weekday', 'hour', 'user_age_level', 'user_star_level', \n",
    "       'user_occupation_id', \n",
    "       'item_city_id_tr', 'item_city_id_pv_cut', 'item_id_tr', 'item_id_pv',\n",
    "       'shop_id_tr', 'shop_id_pv', 'item_brand_id_tr', 'item_brand_id_pv',\n",
    "       'user_occupation_id_pv', 'shop_score_delivery',\n",
    "        'shop_score_description', 'shop_review_positive_rate', 'shop_score_service'      \n",
    "    ]\n",
    "].values\n",
    "#        'shop_score_delivery_scaled', 'shop_score_description_scaled',\n",
    "#        'shop_review_positive_rate_scaled', 'item_city_id_tr_scaled',\n",
    "#        'item_id_tr_scaled', 'shop_id_tr_scaled', 'item_brand_id_tr_scaled',k\n",
    "#        'user_occupation_id_scaled', 'shop_score_service_scaled', \n",
    "        \n",
    "y = offline_df[['is_trade']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##合并维度\n",
    "import numpy as np\n",
    "def mergeToOne(X,X2):\n",
    "    X3=[]\n",
    "    for i in np.arange(X.shape[0]):\n",
    "        tmp=np.array([list(X[i]),list(X2[i])])\n",
    "        X3.append(list(np.hstack(tmp)))\n",
    "    X3=np.array(X3)\n",
    "    return X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size 78268, test set size 70931\n",
      "metrics.classification - 1697: 0.095200737911\n",
      "combined model log_loss 0.095200737911\n",
      "\n",
      "metrics.classification - 1697: 0.0860219639715\n",
      "single model log_loss 0.0860219639715\n",
      "\n",
      "train set size 149199, test set size 68387\n",
      "metrics.classification - 1697: 0.0905225749597\n",
      "combined model log_loss 0.0905225749597\n",
      "\n",
      "metrics.classification - 1697: 0.0850253621848\n",
      "single model log_loss 0.0850253621848\n",
      "\n",
      "train set size 217586, test set size 71199\n",
      "metrics.classification - 1697: 0.0884541214924\n",
      "combined model log_loss 0.0884541214924\n",
      "\n",
      "metrics.classification - 1697: 0.0842971726149\n",
      "single model log_loss 0.0842971726149\n",
      "\n",
      "train set size 288785, test set size 68318\n",
      "metrics.classification - 1697: 0.0848797699255\n",
      "combined model log_loss 0.0848797699255\n",
      "\n",
      "metrics.classification - 1697: 0.0818784075771\n",
      "single model log_loss 0.0818784075771\n",
      "\n",
      "train set size 357103, test set size 63614\n",
      "metrics.classification - 1697: 0.0799545188852\n",
      "combined model log_loss 0.0799545188852\n",
      "\n",
      "metrics.classification - 1697: 0.0770777182651\n",
      "single model log_loss 0.0770777182651\n",
      "\n",
      "train set size 420717, test set size 57421\n",
      "metrics.classification - 1697: 0.07842331366\n",
      "combined model log_loss 0.07842331366\n",
      "\n",
      "metrics.classification - 1697: 0.0759958239903\n",
      "single model log_loss 0.0759958239903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照普通的TimeSeriesSplit, 效果不稳定, 分数非单调提升, 感觉意义不大.\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dtsv = DateTimeSplit(dateSeries=D, fmt=\"%Y-%m-%d\")\n",
    "# tscv = TimeSeriesSplit(n_splits=20)\n",
    "\n",
    "# help(dtsv)\n",
    "# pred_leaf=True\n",
    "\n",
    "layer1_param = {\n",
    "    'learning_rate': 0.3, #默认0.3\n",
    "    'n_estimators': 30, #树的个数\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.5,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree' : 0.6,\n",
    "    'objective': 'binary:logistic', #逻辑回归损失函数\n",
    "    'nthread': 4,  #cpu线程数\n",
    "    'scale_pos_weight' : 1,\n",
    "    'reg_alpha': 1e-05,\n",
    "    'reg_lambda': 1,\n",
    "    'seed': 27\n",
    "}\n",
    "\n",
    "layer2_param = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators':1000,\n",
    "    'max_depth':3,\n",
    "    'min_child_weight':1,\n",
    "    'gamma': 0.5,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'objective': 'binary:logistic', \n",
    "    'nthread': 4, \n",
    "    'scale_pos_weight': 1,\n",
    "    'reg_alpha': 1e-05,\n",
    "    'reg_lambda': 1,\n",
    "    'seed': 27\n",
    "}\n",
    "\n",
    "for train_i, test_i in dtsv.split(X, y):\n",
    "    X_train, y_train = X[train_i], y[train_i]\n",
    "    X_test, y_test = X[test_i], y[test_i]\n",
    "    \n",
    "    X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_train, y_train, test_size=0.6, random_state=0)\n",
    "    \n",
    "    m1 = xgboost.XGBClassifier(**layer1_param)\n",
    "    m1.fit(X_train_1, y_train_1)\n",
    "    new_feature = m1.apply(X_train_2)\n",
    "    print(new_feature)\n",
    "    \n",
    "    \n",
    "    X_train_new2 = mergeToOne(X_train_2, new_feature)\n",
    "    new_feature_test = m1.apply(X_test)\n",
    "    X_test_new = mergeToOne(X_test, new_feature_test)\n",
    "    \n",
    "    \n",
    "    m2 = xgboost.XGBClassifier(**layer2_param)\n",
    "    m2.fit(X_train_new2, y_train_2)\n",
    "    \n",
    "    \n",
    "    m3 = xgboost.XGBClassifier(nthread=4)\n",
    "    m3.fit(X_train, y_train)\n",
    "    \n",
    "    print('train set size %s, test set size %s' % (len(train_i), len(test_i)))\n",
    "    # print('train log_loss %s' % log_loss(y_train, m.predict_proba(X_train)))\n",
    "    print('combined model log_loss %s\\n' % log_loss(y_test, m2.predict_proba(X_test_new)))\n",
    "    print('single model log_loss %s\\n' % log_loss(y_test, m3.predict_proba(X_test)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 xgboost特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先用粗糙的树生成特征叶节点\n",
    "dtsv = DateTimeSplit(dateSeries=offline_df['context_timestamp'], fmt=\"%Y-%m-%d\")\n",
    "\n",
    "for train_i, test_i in dtsv.split(X, y):\n",
    "    X_train, y_train = X[train_i], y[train_i]\n",
    "    X_test, y_test = X[test_i], y[test_i]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  1\n",
       "1  2  1\n",
       "2  3  0\n",
       "3  4  0\n",
       "4  5  1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([1,1,0,0,1])\n",
    "c = pd.DataFrame(np.vstack([a,b]).T)\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [2, 1],\n",
       "       [3, 0],\n",
       "       [4, 0],\n",
       "       [5, 1]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([a,b]).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
