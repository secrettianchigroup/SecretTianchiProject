{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPH第二版实验\n",
    "\n",
    "### 思路\n",
    "\n",
    "*id统计相关*\n",
    "1. item_id, shop_id的交易量, 交易率\n",
    "2. item_id, shop_id7天是否新出现, 3天是否新出现, 昨天是否新出现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "offline_df = pd.read_table('../../round1_ijcai_18_train_20180301.txt', sep=' ')\n",
    "# online_df = pd.read_table('../../round1_ijcai_18_test_a_20180301.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据集统一处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时间处理: 分离天, 星期几, 上中下午/晚上, 小时数\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "def extract_weekday(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.weekday()\n",
    "def extract_hour(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.hour\n",
    "\n",
    "offline_df['date'] = offline_df['context_timestamp'].apply(lambda x: extract_date(x))\n",
    "offline_df['weekday'] = offline_df['context_timestamp'].apply(lambda x: extract_weekday(x))\n",
    "offline_df['hour'] = offline_df['context_timestamp'].apply(lambda x: extract_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../util/time_utils.py\n",
    "\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 3)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 3)\n",
    "\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 7)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'item_brand_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'shop_id', 1)\n",
    "offline_df = getColDupByDate(offline_df, 'date', 'user_id', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计信息附加\n",
    "\n",
    "def getColTradeRate(df, idCol):\n",
    "    rateCol = idCol + '_tr'\n",
    "    pvCol = idCol + '_pv'\n",
    "    try:\n",
    "        del df[rateCol]\n",
    "        del df[pvCol]\n",
    "    except:\n",
    "        pass\n",
    "    a = offline_df.groupby([idCol]).agg({'is_trade':'sum'})\n",
    "    b = offline_df.groupby([idCol]).agg({'is_trade':'size'})\n",
    "    c = a.join(b, lsuffix=\"_sum\", rsuffix=\"_size\")\n",
    "    c[rateCol] = c['is_trade_sum'] / c['is_trade_size']\n",
    "    c[pvCol] = c['is_trade_size']\n",
    "    return offline_df.join(c[[rateCol, pvCol]], on=idCol)\n",
    "    \n",
    "\n",
    "# 各类id的 交易量, 交易率\n",
    "offline_df = getColTradeRate(offline_df, 'item_city_id')\n",
    "offline_df = getColTradeRate(offline_df, 'item_id')\n",
    "offline_df = getColTradeRate(offline_df, 'shop_id')\n",
    "offline_df = getColTradeRate(offline_df, 'item_brand_id')\n",
    "offline_df = getColTradeRate(offline_df, 'user_occupation_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分桶\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "offline_df['item_city_id_pv_cut'] = pd.cut(offline_df['item_city_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['item_id_pv_cut'] = pd.cut(offline_df['item_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['shop_id_pv_cut'] = pd.cut(offline_df['shop_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['item_brand_id_pv_cut'] = pd.cut(offline_df['item_brand_id_pv'].values, bins=10, retbins=False)\n",
    "offline_df['user_occupation_id_pv_cut'] = pd.cut(offline_df['user_occupation_id_pv'].values, bins=10, retbins=False)\n",
    "\n",
    "offline_df['item_city_id_pv_cut'] = le.fit_transform(offline_df['item_city_id_pv_cut'].copy())\n",
    "offline_df['item_id_pv_cut'] = le.fit_transform(offline_df['item_id_pv_cut'].copy())\n",
    "offline_df['shop_id_pv_cut'] = le.fit_transform(offline_df['shop_id_pv_cut'].copy())\n",
    "offline_df['item_brand_id_pv_cut'] = le.fit_transform(offline_df['item_brand_id_pv_cut'].copy())\n",
    "offline_df['user_occupation_id_pv_cut'] = le.fit_transform(offline_df['user_occupation_id_pv_cut'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardization and scaling\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def scale(df, idCol):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df[idCol].values.reshape(-1, 1))\n",
    "    return pd.Series(a.reshape(1, -1)[0])\n",
    "\n",
    "# 令数据再少一些?\n",
    "offline_df['shop_score_service_scaled'] = scale(offline_df, 'shop_score_service')\n",
    "offline_df['shop_score_delivery_scaled'] = scale(offline_df, 'shop_score_delivery')\n",
    "offline_df['shop_score_description_scaled'] = scale(offline_df, 'shop_score_description')\n",
    "offline_df['shop_review_positive_rate_scaled'] = scale(offline_df, 'shop_review_positive_rate')\n",
    "offline_df['item_city_id_tr_scaled'] = scale(offline_df, 'item_city_id_tr')\n",
    "offline_df['item_id_tr_scaled'] = scale(offline_df, 'item_id_tr')\n",
    "offline_df['shop_id_tr_scaled'] = scale(offline_df, 'shop_id_tr')\n",
    "offline_df['item_brand_id_tr_scaled'] = scale(offline_df, 'item_brand_id_tr')\n",
    "offline_df['user_occupation_id_scaled'] = scale(offline_df, 'user_occupation_id_tr')\n",
    "\n",
    "# offline_df = offline_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 生成矩阵数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "used = 78261\n",
    "X = offline_df[\n",
    "    [\n",
    "       'date', \n",
    "        'item_id', 'item_brand_id', 'item_city_id', 'user_id', 'context_page_id', 'context_id', 'shop_id', \n",
    "        'item_id_dup_g_7', 'shop_id_dup_g_7', 'item_brand_id_dup_g_7', 'user_id_dup_g_7',\n",
    "#         'item_id_dup_g_1', 'shop_id_dup_g_1', 'item_brand_id_dup_g_1', 'user_id_dup_g_1',\n",
    "#         'item_id_dup_g_3', 'shop_id_dup_g_3', 'item_brand_id_dup_g_3', 'user_id_dup_g_3',\n",
    "        'item_id_pv_cut', 'shop_id_pv_cut', 'user_occupation_id_pv_cut', \n",
    "         'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', \n",
    "        'user_gender_id', 'weekday', 'hour', 'user_age_level', 'user_star_level', \n",
    "       'user_occupation_id', \n",
    "       'item_city_id_tr', 'item_city_id_pv_cut', 'item_id_tr', 'item_id_pv',\n",
    "       'shop_id_tr', 'shop_id_pv', 'item_brand_id_tr', 'item_brand_id_pv',\n",
    "       'user_occupation_id_pv', 'shop_score_delivery',\n",
    "        'shop_score_description', 'shop_review_positive_rate', 'shop_score_service'      \n",
    "    ]\n",
    "]\n",
    "#        'shop_score_delivery_scaled', 'shop_score_description_scaled',\n",
    "#        'shop_review_positive_rate_scaled', 'item_city_id_tr_scaled',\n",
    "#        'item_id_tr_scaled', 'shop_id_tr_scaled', 'item_brand_id_tr_scaled',k\n",
    "#        'user_occupation_id_scaled', 'shop_score_service_scaled', \n",
    "        \n",
    "y = offline_df[['is_trade']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 kfold 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prehawkmac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-18 to 2018-09-18 is training set, 2018-09-19 is test set, 2018-09-24 is valid set, start training ... \n",
      "0 done, logloss train = 0.08453301421 (compare to previous: 0.08453301421), \n",
      "                    \ttest = 0.0860219639715 (compare to previous: 0.0860219639715), \n",
      "                    \tvalid = 0.0764235695799 (compare to previous: 0.0764235695799)\n",
      "\n",
      "2018-09-18 to 2018-09-19 is training set, 2018-09-20 is test set, 2018-09-24 is valid set, start training ... \n",
      "1 done, logloss train = 0.0850018344811 (compare to previous: 0.000468820271079), \n",
      "                    \ttest = 0.0850253621848 (compare to previous: -0.00099660178662), \n",
      "                    \tvalid = 0.0762046606052 (compare to previous: -0.000218908974612)\n",
      "\n",
      "2018-09-18 to 2018-09-20 is training set, 2018-09-21 is test set, 2018-09-24 is valid set, start training ... \n",
      "2 done, logloss train = 0.0849235743849 (compare to previous: -7.82600961871e-05), \n",
      "                    \ttest = 0.0842971726149 (compare to previous: -0.00072818956999), \n",
      "                    \tvalid = 0.076081847519 (compare to previous: -0.000122813086261)\n",
      "\n",
      "2018-09-18 to 2018-09-21 is training set, 2018-09-22 is test set, 2018-09-24 is valid set, start training ... \n",
      "3 done, logloss train = 0.0847237471564 (compare to previous: -0.000199827228497), \n",
      "                    \ttest = 0.0818784075771 (compare to previous: -0.00241876503772), \n",
      "                    \tvalid = 0.0760316907736 (compare to previous: -5.01567453711e-05)\n",
      "\n",
      "2018-09-18 to 2018-09-22 is training set, 2018-09-23 is test set, 2018-09-24 is valid set, start training ... \n",
      "4 done, logloss train = 0.0841287843948 (compare to previous: -0.000594962761569), \n",
      "                    \ttest = 0.0770777182651 (compare to previous: -0.00480068931199), \n",
      "                    \tvalid = 0.0759447862648 (compare to previous: -8.69045088066e-05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input = X\n",
    "train_input['is_trade'] = y\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "%run ../yuhua/BaseFrame.py\n",
    "\n",
    "score_func = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "it = 0\n",
    "def tatx(train_df, test_df, valid_df):\n",
    "\n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    X_valid = valid_df.copy()\n",
    "    del X_valid['is_trade']\n",
    "    y_valid = valid_df['is_trade']\n",
    "    \n",
    "    if it == 0:\n",
    "        xgb = xgboost.XGBClassifier()\n",
    "        parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "        gs = GridSearchCV(xgb, parameters, n_jobs=5, \n",
    "    else:\n",
    "       \n",
    "    params = {\n",
    "        'nthread': 20\n",
    "    }\n",
    "    m = xgboost.XGBClassifier(**params)\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    l_train = log_loss(y_train, m.predict_proba(X_train))\n",
    "    l_test = log_loss(y_test, m.predict_proba(X_test))\n",
    "    l_valid = log_loss(y_valid, m.predict_proba(X_valid))\n",
    "\n",
    "    return l_train, l_test, l_valid\n",
    "\n",
    "frame = BaseFrame(train_input, 0.05)\n",
    "frame.kfold_by_date('date', tatx)\n",
    "# 0.077189, 0.076092\n",
    "# test = 0.077047  valid = 0.07597\n",
    "# test = 0.077077  valid = 0.075944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(10)\n",
    "indices[2:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build df done\n",
      "(78268, 38) (70931, 1)\n",
      "(70931, 38) (70931, 1)\n",
      "(149199, 38) (68387, 1)\n",
      "(68387, 38) (68387, 1)\n",
      "(217586, 38) (71199, 1)\n",
      "(71199, 38) (71199, 1)\n",
      "(288785, 38) (68318, 1)\n",
      "(68318, 38) (68318, 1)\n",
      "(357103, 38) (63614, 1)\n",
      "(63614, 38) (63614, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import xgboost\n",
    "% run ../util/time_series_split.py\n",
    "\n",
    "\n",
    "dtsv = DateTimeSplit(dateSeries=offline_df['context_timestamp'], fmt=\"%Y-%m-%d\")\n",
    "\n",
    "for train_i, test_i in dtsv.split(X, y):\n",
    "    X_train, y_train = X.values[train_i], y.values[test_i]\n",
    "    X_test, y_test = X.values[test_i], y.values[test_i]\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2018-09-18', 3412720377098676069, 1975590437749032870, ..., 1.0,\n",
       "        1.0, 1.0],\n",
       "       ['2018-09-18', 3412720377098676069, 1975590437749032870, ..., 1.0,\n",
       "        1.0, 1.0],\n",
       "       ['2018-09-18', 3412720377098676069, 1975590437749032870, ..., 1.0,\n",
       "        1.0, 1.0],\n",
       "       ..., \n",
       "       ['2018-09-18', 5251805314991883348, 7838285046767229711, ...,\n",
       "        0.9518072289156628, 0.953125, 0.9556626506024096],\n",
       "       ['2018-09-18', 5251805314991883348, 7838285046767229711, ...,\n",
       "        0.9518072289156628, 0.953125, 0.9556626506024096],\n",
       "       ['2018-09-18', 5251805314991883348, 7838285046767229711, ...,\n",
       "        0.9518072289156628, 0.953125, 0.9556626506024096]], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(len(X))\n",
    "indices[ind]\n",
    "X.values[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
