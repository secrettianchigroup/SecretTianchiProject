{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import ffmyh\n",
    "import time\n",
    "import math\n",
    "# from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "# from fastFM.datasets import make_user_item_regression\n",
    "# from fastFM import als\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_org = pd.read_table('/Users/yuhua/先验CTR模型线上观察/Alimama比赛/round1_ijcai_18_train_20180301.txt',sep=' ')\n",
    "test_df_org = pd.read_table('/Users/yuhua/先验CTR模型线上观察/Alimama比赛/round1_ijcai_18_test_a_20180301.txt',sep=' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_org.columns\n",
    "\n",
    "train_df = train_df_org.copy().set_index(\"context_id\")\n",
    "test_df = test_df_org.copy().set_index(\"context_id\")\n",
    "\n",
    "test_df[\"is_trade\"] = -1\n",
    "train_df = train_df.append(test_df)\n",
    "# print len(train_df)\n",
    "# print train_df[[\"is_trade\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2018-09-24': 6, '2018-09-25': 7, '2018-09-20': 2, '2018-09-21': 3, '2018-09-22': 4, '2018-09-23': 5, '2018-09-19': 1, '2018-09-18': 0}\n"
     ]
    }
   ],
   "source": [
    "# 时间处理: 分离天, 星期几, 上中下午/晚上, 小时数\n",
    "# date最终不使用，直接用day(第 0 - 7 天)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "def extract_weekday(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.weekday()\n",
    "def extract_hour(x):\n",
    "    d = datetime.fromtimestamp(x)\n",
    "    return d.hour\n",
    "\n",
    "train_df['date'] = train_df['context_timestamp'].apply(lambda x: extract_date(x))\n",
    "train_df['weekday'] = train_df['context_timestamp'].apply(lambda x: extract_weekday(x))\n",
    "train_df['hour'] = train_df['context_timestamp'].apply(lambda x: extract_hour(x))\n",
    "\n",
    "m = {}\n",
    "for idx, date in enumerate(sorted(train_df['date'].unique())):\n",
    "    m[date]=idx\n",
    "print(m)\n",
    "\n",
    "for idx, d in enumerate(sorted(train_df['date'].unique())):\n",
    "    train_df[\"day\"] = train_df['date'].map(lambda x: m[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拆解复杂类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_icl_map ... \n",
      "get_ipl_map ... \n"
     ]
    }
   ],
   "source": [
    "#简化list等复杂类型的结构\n",
    "#item_category_list全展开\n",
    "#item_property_list全展开取频率>0.05的数据\n",
    "#predict_category_property会计算跟item prop和cate的余弦相似度\n",
    "#具体是把两个list的数据拼成cate:-1和cate:prop两种方式拼成一个字符串再跟predict_category_property的数据计算相似度\n",
    "def get_icl_map(df):\n",
    "    print(\"get_icl_map ... \")\n",
    "    dfX = df.copy()\n",
    "    dfX = dfX['item_category_list'].str.split(';', expand=True)\n",
    "\n",
    "#     s = sorted(list(dfX[0].unique()) + list(dfX[1].unique()) + list(dfX[2].unique()))\n",
    "    m = {}\n",
    "    for i in dfX[0].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"1\"\n",
    "    \n",
    "    for i in dfX[1].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"2\"\n",
    "    \n",
    "    for i in dfX[2].unique():\n",
    "        if i == None:\n",
    "            continue\n",
    "        m[i] = \"3\"\n",
    "    return m\n",
    "\n",
    "def get_ipl_map(df):\n",
    "    print(\"get_ipl_map ... \")\n",
    "    df1 = df.copy()\n",
    "    dfX = df1.copy()['item_property_list'].str.split(';')\n",
    "    dfX = pd.DataFrame(dfX)\n",
    "    \n",
    "    m = collections.defaultdict(float)\n",
    "    idx = 0\n",
    "    for _, row in dfX.iterrows():\n",
    "        for i in row[0]:\n",
    "            m[i] += 1\n",
    "    \n",
    "    ll = len(dfX)\n",
    "    for k,v in m.items():\n",
    "        m[k] = v / ll\n",
    "    return m\n",
    "\n",
    "def process_complex_types(dfX, icl_map, ipl_map):\n",
    "    def filter_unless_cate(arr):\n",
    "        ret = []\n",
    "        for i in arr:\n",
    "            if i in icl_map:\n",
    "                ret.append(i)\n",
    "        if len(ret) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def filter_unless_prop(arr):\n",
    "        ret = []\n",
    "        for i in arr:\n",
    "            freq = ipl_map.get(i, 0.)\n",
    "            if freq > 0.05:\n",
    "                ret.append(i)\n",
    "        if len(ret) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def unique_list(arr):\n",
    "        return list(set(arr))\n",
    "    \n",
    "    #{cate}:-1命中则为1分\n",
    "    #{cate}:{prop}命中则为2分\n",
    "    #后期优化权重\n",
    "    def inner_product_recall_items(line):\n",
    "        line = line.split(\"|\")\n",
    "        item_category_list = unique_list(line[0].split(\";\"))\n",
    "        item_property_list = unique_list(line[1].split(\";\"))\n",
    "        \n",
    "        whole_combines = {}\n",
    "        for cate in item_category_list:\n",
    "            tmp = cate+\":\"+\"-1\"\n",
    "            whole_combines[tmp] = 1\n",
    "            for prop in item_property_list:\n",
    "                tmp = cate+\":\"+prop\n",
    "                whole_combines[tmp] = 2\n",
    "        \n",
    "        \n",
    "                \n",
    "        predict_category_property = unique_list(line[2].split(\";\"))\n",
    "        product = 0.\n",
    "        item_vec_len = math.sqrt(len(whole_combines))\n",
    "        user_vec_len = math.sqrt(len(predict_category_property))\n",
    "        for item in predict_category_property:\n",
    "            #x1 == 1\n",
    "            #y1 == 1\n",
    "            #x1*y1 == 1\n",
    "            #x2 == 0\n",
    "            #y2 == 1\n",
    "            #x2*y2 == 0\n",
    "            #所以product由x决定 += 1/0\n",
    "            product += whole_combines.get(item, 0)\n",
    "        \n",
    "        return product/(item_vec_len*user_vec_len)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"processing predict_category_property ...\")\n",
    "#     dfX['predict_category_property'] = dfX['predict_category_property'].str.split(';').map(lambda x: [i.split(\":\")[0] for i in x]).map(filter_unless_cate)\n",
    "    \n",
    "    dfX['predict_richness'] =  dfX['predict_category_property'].map(lambda x: 0 if len(x.strip()) == 0 else len(x.split(\";\")))\n",
    "    dfX['predict_category_property'] = dfX['item_category_list']+\"|\"+dfX['item_property_list']+\"|\"+dfX['predict_category_property']\n",
    "    dfX['predict_category_property'] = dfX['predict_category_property'].map(inner_product_recall_items)\n",
    "    \n",
    "    print(\"processing item_property_list ...\")\n",
    "    dfX['item_property_richness'] = dfX['item_property_list'].map(lambda x: 0 if len(x.strip()) == 0 else len(x.split(\";\")))\n",
    "    dfX['item_property_list'] = dfX['item_property_list'].str.split(';').map(filter_unless_prop).map(unique_list)\n",
    "    \n",
    "    print(\"processing item_category_list ...\")\n",
    "    dfX['item_category_list'] = dfX['item_category_list'].str.split(';')\n",
    "    \n",
    "    print \"generating item_category_1, item_category_2 ...\"\n",
    "#     dfX['item_category_list01'] = dfX['item_category_list'].map(lambda x:x[0] if x != None and len(x) > 0 else None)\n",
    "    dfX['item_category_1'] = dfX['item_category_list'].map(lambda x:x[1] if x != None and len(x) > 1 else None)\n",
    "    dfX['item_category_2'] = dfX['item_category_list'].map(lambda x:x[2] if x != None and len(x) > 2 else None)\n",
    "    \n",
    "    return dfX\n",
    "\n",
    "\n",
    "\n",
    "# aaa = process_complex_types(train_df.copy(), get_icl_map(train_df), get_ipl_map(train_df))\n",
    "\n",
    "train_df = process_complex_types(train_df, get_icl_map(train_df), get_ipl_map(train_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转化属性增加trade_rate + trade_pv（按前一天）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计信息附加\n",
    "\n",
    "def calcTVTransform(df, key, key_y, filter_src, filter_dst, smoothing = 5, mean0=None):\n",
    "    if mean0 is None:\n",
    "        #计算目标的平均值做平缓用\n",
    "        mean0 = df.ix[filter_src, key_y].mean()\n",
    "        print(\"mean0:\", mean0)\n",
    "    \n",
    "    #取出key的所有值\n",
    "    df['_key1'] = df[key].astype('category').values.codes\n",
    "    \n",
    "    \n",
    "    #取出用于计算的源（后面聚合掉就没有顺序可言了）\n",
    "    df_key1_y = df.ix[filter_src, ['_key1', key_y]]\n",
    "    \n",
    "    #根据key的取值去聚合key_y的总数和总和，用户计算rate和count\n",
    "    grp1 = df_key1_y.groupby(['_key1'])\n",
    "    sum1 = grp1[key_y].aggregate(np.sum)\n",
    "    cnt1 = grp1[key_y].aggregate(np.size)\n",
    "    \n",
    "    vn_sum = 'sum_' + key\n",
    "    vn_cnt = 'cnt_' + key\n",
    "    \n",
    "    #取出dst（带序列）的所有key\n",
    "    v_codes = df.ix[filter_dst, '_key1']\n",
    "    \n",
    "    #得到_sum,_cnt，按dst的序列\n",
    "    _sum = sum1[v_codes].values\n",
    "    _cnt = cnt1[v_codes].values\n",
    "    _cnt[np.isnan(_sum)] = 0    \n",
    "    _sum[np.isnan(_sum)] = 0\n",
    "    \n",
    "    r = {}\n",
    "    r['exp'] = (_sum + smoothing * mean0)/(_cnt + smoothing)\n",
    "    r['cnt'] = _cnt\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算前一天的交易率set到下一天，第0天用回自己\n",
    "# tmp = train_df.copy()\n",
    "tmp = train_df\n",
    "\n",
    "add_count = False\n",
    "# window = 2\n",
    "\n",
    "exp = \"exp_d_\"\n",
    "cnt = \"cnt_d_\"\n",
    "\n",
    "\n",
    "exp_numerical = {}\n",
    "cnt_numerical = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_df\n",
    "k = \"day\"\n",
    "exp_k = \"exp_d_day\"\n",
    "for day in xrange(0,7):\n",
    "    cal_day = day\n",
    "    set_day = day\n",
    "\n",
    "    print(\"cal %s trade_rate cnt %s set to %s\" % (k,cal_day, set_day))\n",
    "\n",
    "    #start_d - day(不含day)用于计算，结果赋值到day上\n",
    "    days1 = (tmp.day.values == cal_day)\n",
    "    days2 = (tmp.day.values == set_day)\n",
    "    ret = calcTVTransform(tmp, k, 'is_trade', days1, days2, 0)\n",
    "\n",
    "    tmp.loc[tmp.day.values == day, exp_k] = ret[\"exp\"]\n",
    "\n",
    "    exp_numerical[exp_k]=1\n",
    "\n",
    "tmp.loc[tmp.day.values == 7, exp_k] = 0.0169\n",
    "\n",
    "tmp[[exp_k, \"day\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#此处应该处理按天为梯度的数据\n",
    "for k in ['item_price_level',\n",
    "'item_sales_level',\n",
    "'item_collected_level',\n",
    "'item_pv_level',\n",
    "'user_gender_id',\n",
    "'user_age_level',\n",
    "'user_occupation_id',\n",
    "'user_star_level',\n",
    "'context_page_id',\n",
    "'shop_review_num_level',\n",
    "'shop_star_level',\n",
    "         \n",
    "\n",
    "\"item_brand_id\",\n",
    "\"item_city_id\",\n",
    "\"item_id\",\n",
    "\"shop_id\"\n",
    "         ]:\n",
    "    exp_k = exp+k\n",
    "    cnt_k = cnt+k\n",
    "    for day in xrange(0,8):\n",
    "#         start_d = max(day - window, 0)\n",
    "#         end_d = max(day - 1,0)\n",
    "        cal_day = max(day - 1, 0)\n",
    "        set_day = day\n",
    "\n",
    "        print(\"cal %s trade_rate cnt %s set to %s\" % (k,cal_day, set_day))\n",
    "        \n",
    "        #start_d - day(不含day)用于计算，结果赋值到day上\n",
    "        days1 = (tmp.day.values == cal_day)\n",
    "        days2 = (tmp.day.values == set_day)\n",
    "        ret = calcTVTransform(tmp, k, 'is_trade', days1, days2)\n",
    "            \n",
    "        tmp.loc[tmp.day.values == day, exp_k] = ret[\"exp\"]\n",
    "        \n",
    "        exp_numerical[exp_k]=1\n",
    "        if add_count:\n",
    "            cnt_numerical[cnt_k]=1\n",
    "            tmp.loc[tmp.day.values == day, cnt_k] = ret[\"cnt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_numerical = exp_numerical.keys()\n",
    "cnt_numerical = cnt_numerical.keys()\n",
    "\n",
    "print(exp_numerical)\n",
    "print(cnt_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标记训练用的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#标记训练数据\n",
    "%run FeatureProcess.py\n",
    "import copy\n",
    "target=\"is_trade\"\n",
    "                          \n",
    "                          \n",
    "                        \n",
    "categorical=[  \n",
    "                'item_price_level',\n",
    "                'item_sales_level',\n",
    "                'item_collected_level',\n",
    "                'item_pv_level',\n",
    "                'user_gender_id',\n",
    "                'user_age_level',\n",
    "                'user_occupation_id',\n",
    "                'user_star_level',\n",
    "                'context_page_id',\n",
    "                'shop_review_num_level',\n",
    "                'shop_star_level',\n",
    "                \"weekday\",\n",
    "                \"hour\",\n",
    "    \n",
    "                'item_category_1',\n",
    "                'item_category_2',\n",
    "                \n",
    "                \"item_brand_id\",\n",
    "                \"item_city_id\",\n",
    "                \"item_id\",\n",
    "                \"shop_id\"\n",
    "            ]\n",
    "\n",
    "numerical=[     'shop_review_positive_rate',\n",
    "                'shop_score_service',\n",
    "                'shop_score_delivery',\n",
    "                'shop_score_description',\n",
    "                'predict_category_property',\n",
    "                'predict_richness',\n",
    "                'item_property_richness'\n",
    "                   \n",
    "#               ]\n",
    "          ]+exp_numerical\n",
    "\n",
    "listype = [\n",
    "    'item_property_list', \n",
    "#     'item_category_list'\n",
    "]\n",
    "\n",
    "                    \n",
    "            \n",
    "\n",
    "cols_tool = filter_on_cols(target, categorical, numerical, listype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[cols_tool.get_raw_numerical_cols()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平滑处理连续型数据最后对复杂类型做onehot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_review_positive_rate</th>\n",
       "      <th>shop_score_service</th>\n",
       "      <th>shop_score_delivery</th>\n",
       "      <th>shop_score_description</th>\n",
       "      <th>predict_category_property</th>\n",
       "      <th>predict_richness</th>\n",
       "      <th>item_property_richness</th>\n",
       "      <th>exp_d_user_age_level</th>\n",
       "      <th>exp_d_item_pv_level</th>\n",
       "      <th>exp_d_shop_review_num_level</th>\n",
       "      <th>exp_d_item_sales_level</th>\n",
       "      <th>exp_d_user_occupation_id</th>\n",
       "      <th>exp_d_item_city_id</th>\n",
       "      <th>exp_d_item_brand_id</th>\n",
       "      <th>exp_d_user_star_level</th>\n",
       "      <th>exp_d_item_price_level</th>\n",
       "      <th>exp_d_day</th>\n",
       "      <th>exp_d_item_id</th>\n",
       "      <th>exp_d_shop_star_level</th>\n",
       "      <th>exp_d_item_collected_level</th>\n",
       "      <th>exp_d_user_gender_id</th>\n",
       "      <th>exp_d_context_page_id</th>\n",
       "      <th>exp_d_shop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "      <td>496509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.971535</td>\n",
       "      <td>0.970906</td>\n",
       "      <td>0.975237</td>\n",
       "      <td>0.164666</td>\n",
       "      <td>0.293092</td>\n",
       "      <td>0.302926</td>\n",
       "      <td>0.688612</td>\n",
       "      <td>0.101611</td>\n",
       "      <td>0.218402</td>\n",
       "      <td>0.511220</td>\n",
       "      <td>0.836325</td>\n",
       "      <td>0.098886</td>\n",
       "      <td>0.081723</td>\n",
       "      <td>0.341940</td>\n",
       "      <td>0.166898</td>\n",
       "      <td>0.606917</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>0.153373</td>\n",
       "      <td>0.538369</td>\n",
       "      <td>0.699759</td>\n",
       "      <td>0.687828</td>\n",
       "      <td>0.071759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.009858</td>\n",
       "      <td>0.014145</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.016028</td>\n",
       "      <td>0.117458</td>\n",
       "      <td>0.138144</td>\n",
       "      <td>0.107984</td>\n",
       "      <td>0.115209</td>\n",
       "      <td>0.014680</td>\n",
       "      <td>0.044658</td>\n",
       "      <td>0.162354</td>\n",
       "      <td>0.080191</td>\n",
       "      <td>0.042041</td>\n",
       "      <td>0.074096</td>\n",
       "      <td>0.052908</td>\n",
       "      <td>0.076621</td>\n",
       "      <td>0.361303</td>\n",
       "      <td>0.077242</td>\n",
       "      <td>0.031012</td>\n",
       "      <td>0.086564</td>\n",
       "      <td>0.145956</td>\n",
       "      <td>0.124316</td>\n",
       "      <td>0.079743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.992852</td>\n",
       "      <td>0.966693</td>\n",
       "      <td>0.966017</td>\n",
       "      <td>0.969586</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.626489</td>\n",
       "      <td>0.094769</td>\n",
       "      <td>0.200471</td>\n",
       "      <td>0.399855</td>\n",
       "      <td>0.812573</td>\n",
       "      <td>0.070562</td>\n",
       "      <td>0.028698</td>\n",
       "      <td>0.326793</td>\n",
       "      <td>0.121504</td>\n",
       "      <td>0.120499</td>\n",
       "      <td>0.012521</td>\n",
       "      <td>0.132899</td>\n",
       "      <td>0.500887</td>\n",
       "      <td>0.674225</td>\n",
       "      <td>0.666383</td>\n",
       "      <td>0.015845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972620</td>\n",
       "      <td>0.971860</td>\n",
       "      <td>0.978705</td>\n",
       "      <td>0.156556</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.684715</td>\n",
       "      <td>0.102018</td>\n",
       "      <td>0.214012</td>\n",
       "      <td>0.513787</td>\n",
       "      <td>0.828809</td>\n",
       "      <td>0.087382</td>\n",
       "      <td>0.076188</td>\n",
       "      <td>0.348955</td>\n",
       "      <td>0.141488</td>\n",
       "      <td>0.772863</td>\n",
       "      <td>0.038448</td>\n",
       "      <td>0.151369</td>\n",
       "      <td>0.537175</td>\n",
       "      <td>0.714830</td>\n",
       "      <td>0.695428</td>\n",
       "      <td>0.048508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978041</td>\n",
       "      <td>0.977194</td>\n",
       "      <td>0.983789</td>\n",
       "      <td>0.235412</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.748742</td>\n",
       "      <td>0.106222</td>\n",
       "      <td>0.247507</td>\n",
       "      <td>0.588760</td>\n",
       "      <td>0.866525</td>\n",
       "      <td>0.117639</td>\n",
       "      <td>0.106783</td>\n",
       "      <td>0.373371</td>\n",
       "      <td>0.205879</td>\n",
       "      <td>0.891105</td>\n",
       "      <td>0.085545</td>\n",
       "      <td>0.180926</td>\n",
       "      <td>0.576776</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.103168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shop_review_positive_rate  shop_score_service  shop_score_delivery  \\\n",
       "count              496509.000000       496509.000000        496509.000000   \n",
       "mean                    0.994898            0.971535             0.970906   \n",
       "std                     0.009858            0.014145             0.014084   \n",
       "min                     0.000000            0.000000             0.000000   \n",
       "25%                     0.992852            0.966693             0.966017   \n",
       "50%                     1.000000            0.972620             0.971860   \n",
       "75%                     1.000000            0.978041             0.977194   \n",
       "max                     1.000000            1.000000             1.000000   \n",
       "\n",
       "       shop_score_description  predict_category_property  predict_richness  \\\n",
       "count           496509.000000              496509.000000     496509.000000   \n",
       "mean                 0.975237                   0.164666          0.293092   \n",
       "std                  0.016028                   0.117458          0.138144   \n",
       "min                  0.000000                   0.000000          0.000000   \n",
       "25%                  0.969586                   0.083333          0.153846   \n",
       "50%                  0.978705                   0.156556          0.307692   \n",
       "75%                  0.983789                   0.235412          0.384615   \n",
       "max                  1.000000                   1.000000          1.000000   \n",
       "\n",
       "       item_property_richness  exp_d_user_age_level  exp_d_item_pv_level  \\\n",
       "count           496509.000000         496509.000000        496509.000000   \n",
       "mean                 0.302926              0.688612             0.101611   \n",
       "std                  0.107984              0.115209             0.014680   \n",
       "min                  0.000000              0.000000             0.000000   \n",
       "25%                  0.244681              0.626489             0.094769   \n",
       "50%                  0.287234              0.684715             0.102018   \n",
       "75%                  0.351064              0.748742             0.106222   \n",
       "max                  1.000000              1.000000             1.000000   \n",
       "\n",
       "       exp_d_shop_review_num_level  exp_d_item_sales_level  \\\n",
       "count                496509.000000           496509.000000   \n",
       "mean                      0.218402                0.511220   \n",
       "std                       0.044658                0.162354   \n",
       "min                       0.000000                0.000000   \n",
       "25%                       0.200471                0.399855   \n",
       "50%                       0.214012                0.513787   \n",
       "75%                       0.247507                0.588760   \n",
       "max                       1.000000                1.000000   \n",
       "\n",
       "       exp_d_user_occupation_id  exp_d_item_city_id  exp_d_item_brand_id  \\\n",
       "count             496509.000000       496509.000000        496509.000000   \n",
       "mean                   0.836325            0.098886             0.081723   \n",
       "std                    0.080191            0.042041             0.074096   \n",
       "min                    0.000000            0.000000             0.000000   \n",
       "25%                    0.812573            0.070562             0.028698   \n",
       "50%                    0.828809            0.087382             0.076188   \n",
       "75%                    0.866525            0.117639             0.106783   \n",
       "max                    1.000000            1.000000             1.000000   \n",
       "\n",
       "       exp_d_user_star_level  exp_d_item_price_level      exp_d_day  \\\n",
       "count          496509.000000           496509.000000  496509.000000   \n",
       "mean                0.341940                0.166898       0.606917   \n",
       "std                 0.052908                0.076621       0.361303   \n",
       "min                 0.000000                0.000000       0.000000   \n",
       "25%                 0.326793                0.121504       0.120499   \n",
       "50%                 0.348955                0.141488       0.772863   \n",
       "75%                 0.373371                0.205879       0.891105   \n",
       "max                 1.000000                1.000000       1.000000   \n",
       "\n",
       "       exp_d_item_id  exp_d_shop_star_level  exp_d_item_collected_level  \\\n",
       "count  496509.000000          496509.000000               496509.000000   \n",
       "mean        0.064086               0.153373                    0.538369   \n",
       "std         0.077242               0.031012                    0.086564   \n",
       "min         0.000000               0.000000                    0.000000   \n",
       "25%         0.012521               0.132899                    0.500887   \n",
       "50%         0.038448               0.151369                    0.537175   \n",
       "75%         0.085545               0.180926                    0.576776   \n",
       "max         1.000000               1.000000                    1.000000   \n",
       "\n",
       "       exp_d_user_gender_id  exp_d_context_page_id  exp_d_shop_id  \n",
       "count         496509.000000          496509.000000  496509.000000  \n",
       "mean               0.699759               0.687828       0.071759  \n",
       "std                0.145956               0.124316       0.079743  \n",
       "min                0.000000               0.000000       0.000000  \n",
       "25%                0.674225               0.666383       0.015845  \n",
       "50%                0.714830               0.695428       0.048508  \n",
       "75%                0.745833               0.747943       0.103168  \n",
       "max                1.000000               1.000000       1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据的预处理\n",
    "#double系列(例如好评率)的数据存在一个问题，没有考虑到评论量，所以乘了一个评论level，类似 好评率*评论数(分桶了) = 好评量(分桶了)\n",
    "#fillempty把-1设置成-0.01，配合mov2pos使用\n",
    "#mov2pos把有<0的数据都集体-min，如果一个数据是0-1，有-1出现的时候，上面设置成了-0.01，所以这列数据真实是-0.01 - 1,然后我会集体-(-0.01)\n",
    "#norm用了最大值最小值norm\n",
    "%run FeatureProcess.py\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def bucketizer(df, key, bins):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    prefix=\"*BUK*_\"\n",
    "    df[prefix+key] = pd.cut(df[key].values, bins=5, retbins=False)\n",
    "    df[prefix+key] = le.fit_transform(df[prefix+key].copy())\n",
    "    return df\n",
    "\n",
    "\n",
    "def layering(df, key, by_key):\n",
    "    uniqv = df[by_key].unique()\n",
    "    keys = []\n",
    "    for v in uniqv:\n",
    "        indexer = (df[by_key] == v)\n",
    "        new_key = \"*%sLAY*_%s\" % (str(v), key)\n",
    "        df.loc[indexer, new_key] = df[indexer][key]\n",
    "        keys.append(new_key)\n",
    "    return df, keys\n",
    "\n",
    "# #分桶+打散\n",
    "# print(\"bucketizer ... \")\n",
    "# train_df = bucketizer(train_df, \"shop_review_num_level\", 5)\n",
    "\n",
    "# print(\"layering ... \")\n",
    "# new_n = []\n",
    "# for n in numerical:\n",
    "#     train_df, keys = layering(train_df, n, '*BUK*_shop_review_num_level')\n",
    "#     new_n += keys\n",
    "\n",
    "\n",
    "\n",
    "featProc = FeatureProcess(target=target, categorical=categorical, numerical=numerical, listype = listype)\n",
    "# for n in numerical:\n",
    "#     featProc.addLayeringOrgKeys(n)\n",
    "# train_df[\"shop_review_num_level\"] = train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_review_positive_rate\"] = train_df[\"shop_review_positive_rate\"]*train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_score_service\"] = train_df[\"shop_score_service\"]*train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_score_delivery\"] = train_df[\"shop_score_delivery\"]*train_df[\"shop_review_num_level\"]\n",
    "# train_df[\"shop_score_description\"] = train_df[\"shop_score_description\"]*train_df[\"shop_review_num_level\"]\n",
    "\n",
    "train_df = featProc.fillempty(train_df, -0.01)\n",
    "# train_df = featProc.mov2pos(train_df)\n",
    "train_df = featProc.mov2mean(train_df)\n",
    "train_df = featProc.norm(train_df)\n",
    "train_df[cols_tool.get_raw_numerical_cols()].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toOneHot ...\n",
      "toOneHotList ...\n"
     ]
    }
   ],
   "source": [
    "#onehot没啥好说的，就是onehot，toOneHotList实现有点不同分开了而已\n",
    "\n",
    "\n",
    "train_df = featProc.toOneHot(train_df)\n",
    "train_df = featProc.toOneHotList(train_df)\n",
    "\n",
    "# train_df = featProc.cacheRun(featProc.toOneHot, train_df)\n",
    "\n",
    "for c in train_df.columns:\n",
    "    print(c)\n",
    "\n",
    "print train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ffmyh.py\n",
    "\n",
    "    \n",
    "# def train_and_test_combine(train_df, test_df, res_df):\n",
    "    \n",
    "#     X_train = train_df.copy()\n",
    "#     del X_train['is_trade']\n",
    "#     y_train = train_df['is_trade']\n",
    "    \n",
    "#     X_test = test_df.copy()\n",
    "#     del X_test['is_trade']\n",
    "#     y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "    \n",
    "#     model = xgboost.XGBClassifier(nthread=7,max_depth=5)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     y_prev_predict_train = pd.DataFrame(model.predict_proba(X_train))\n",
    "#     y_prev_predict_train[\"idx\"] = X_train.index\n",
    "#     y_prev_predict_train = y_prev_predict_train.set_index(\"idx\")\n",
    "#     X_train[\"y_prev_predict\"] = y_prev_predict_train[1]\n",
    "    \n",
    "#     y_prev_predict_test = pd.DataFrame(model.predict_proba(X_test))\n",
    "#     y_prev_predict_test[\"idx\"] = X_test.index\n",
    "#     y_prev_predict_test = y_prev_predict_test.set_index(\"idx\")\n",
    "#     X_test[\"y_prev_predict\"] = y_prev_predict_test[1]\n",
    "    \n",
    "    \n",
    "# #     X_train = X_train.fillna(0)\n",
    "# #     X_test = X_test.fillna(0)\n",
    "    \n",
    "    \n",
    "#     model = LogisticRegression(max_iter=1000)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "#     train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "#     test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "#     del train_df\n",
    "#     del test_df\n",
    "    \n",
    "#     del X_train\n",
    "#     del y_train\n",
    "#     del X_test\n",
    "#     del y_test\n",
    "    \n",
    "#     return train_lls, test_lls\n",
    "    \n",
    "\n",
    "def train_and_test_xgboost(train_df, test_df, res_df):    \n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "#     model = xgboost.XGBClassifier(reg_lambda=1.5,learning_rate=0.1,reg_alpha=0,nthread=7,n_estimators=100,max_depth=10)\n",
    "    model = xgboost.XGBClassifier(subsample=0.9,colsample_bytree=0.9,n_estimators=500,max_depth=7,nthread=7)\n",
    "\n",
    "#     model = xgboost.XGBClassifier(subsample=0.9,colsample_bytree=0.9,n_estimators=50,max_depth=3,nthread=7)\n",
    "    model.fit(X_train, y_train)\n",
    "    fig, ax = plt.subplots(figsize=(12,18))\n",
    "    xgboost.plot_importance(model,ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"xgboost_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "def train_and_test_lr(train_df, test_df, res_df):    \n",
    "    X_train = train_df.copy().fillna(0)\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy().fillna(0)\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = LogisticRegression(C=1.0, n_jobs=7,tol=1e-6, max_iter=2000)\n",
    "    model.fit(X_train.values, y_train)\n",
    "#     for idx, val in enumerate(list(X_train.columns)):\n",
    "#         print(\"%s=%s\" %(val,list(model.coef_[0])[idx]))\n",
    "\n",
    "\n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        res_df = res_df.copy().fillna(0)\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"lr_res.csv\",index=False,sep=' ')\n",
    "    \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "def train_and_test_randomforest(train_df, test_df, res_df):    \n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "#     rf = RandomForestClassifier(n_estimators=32, max_depth=40, min_samples_split=100, min_samples_leaf=10,  criterion='entropy',\n",
    "#                         max_features=8, verbose = 1,  bootstrap=False, n_jobs=10)\n",
    "#     RandomForestClassifier(criterion='entropy',n_estimators=100,n_jobs=15)\n",
    "\n",
    "    model = RandomForestClassifier(n_jobs=7, n_estimators=100, max_depth=10, min_samples_split=100, min_samples_leaf=10,  criterion='entropy', max_features=8, verbose = 1,  bootstrap=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    ft_w = pd.DataFrame(model.feature_importances_, columns=[\"weights\"], index = list(X_train.columns))\n",
    "    print ft_w.sort_values(\"weights\", ascending=False)\n",
    "#     fig, ax = plt.subplots(figsize=(12,18))\n",
    "#     xgboost.plot_importance(model,ax=ax)\n",
    "#     plt.show()\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"randomforest_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "def train_and_test_gbdt(train_df, test_df, res_df):\n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=100, max_depth=10, min_samples_split=100, min_samples_leaf=10, max_features=4)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"gbdt_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "\n",
    "def train_and_test_lgb(train_df, test_df, res_df, target, features, categorical):\n",
    "    X_train = train_df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = train_df['is_trade']\n",
    "    \n",
    "    X_test = test_df.copy()\n",
    "    del X_test['is_trade']\n",
    "    y_test = test_df['is_trade']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     test['lgb_predict'] = clf.predict_proba(test[features],)[:, 1]\n",
    "#     print(log_loss(test[target], test['lgb_predict']))\n",
    "\n",
    "    \n",
    "    model = lgb.LGBMClassifier(num_leaves=10, max_depth=7, n_estimators=80, n_jobs=20)\n",
    "    model.fit(X_train, y_train, feature_name=features,categorical_feature=categorical)\n",
    "    \n",
    "    ft_w = pd.DataFrame(model.feature_importances_, columns=[\"weights\"], index = list(X_train.columns))\n",
    "    print ft_w\n",
    "#     fig, ax = plt.subplots(figsize=(12,18))\n",
    "#     xgboost.plot_importance(model,ax=ax)\n",
    "#     plt.show()\n",
    "    \n",
    "    train_lls = log_loss(y_train,model.predict_proba(X_train))\n",
    "    test_lls = log_loss(y_test,model.predict_proba(X_test))\n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"lgb_res.csv\",index=False,sep=' ')\n",
    "        \n",
    "    del train_df\n",
    "    del test_df\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "\n",
    "def train_and_test_FFM(train_df, test_df, res_df):\n",
    "    global cols_tool\n",
    "    \n",
    "    model = FFM(target=cols_tool.target, categorical=cols_tool.categorical, numerical=cols_tool.numerical, listype=cols_tool.listype,\n",
    "                                        reg_param = 0.0004,\n",
    "                                        k = 4,\n",
    "                                        iter_max = 500,\n",
    "                                        learing_rate = 0.1,\n",
    "                                        threads = 7,\n",
    "                                        auto_stop = False,\n",
    "                                        quiet = False,\n",
    "                                        no_norm = False)\n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(train_df, test_df)\n",
    "    \n",
    "    train_lls = log_loss(train_df[\"is_trade\"],model.predict_proba(train_df))\n",
    "    test_lls = log_loss(test_df[\"is_trade\"],model.predict_proba(test_df))\n",
    "    \n",
    "    \n",
    "    if type(res_df) != type(None):\n",
    "        del res_df[\"is_trade\"]\n",
    "        res_df = res_df.reset_index(drop=True)\n",
    "        instance_id_list = res_df[[\"instance_id\"]]\n",
    "        del res_df[\"instance_id\"]\n",
    "        predicted_score = pd.DataFrame(model.predict_proba(res_df))\n",
    "        \n",
    "        instance_id_list[\"predicted_score\"] = predicted_score[1]\n",
    "        instance_id_list.to_csv(\"ffm.csv\",index=False,sep=' ')\n",
    "        \n",
    "    \n",
    "    return train_lls, test_lls\n",
    "\n",
    "\n",
    "def select_best_features(df, precent=0.8):\n",
    "    from sklearn.feature_selection import SelectKBest,chi2 \n",
    "    df = df.copy()\n",
    "    \n",
    "    for u in df.columns:\n",
    "        df[df[u] == -1] = 0\n",
    "\n",
    "    X_train = df.copy()\n",
    "    del X_train['is_trade']\n",
    "    y_train = df['is_trade']\n",
    "    \n",
    "    c = int(len(X_train.columns) * precent)\n",
    "    skb = SelectKBest(chi2,k=c)\n",
    "    skb.fit_transform(X_train,y_train)\n",
    "    \n",
    "    old_fea = list(X_train.columns)\n",
    "    new_fea = []\n",
    "    for idx, b in enumerate(list(skb.get_support())):\n",
    "        if b:\n",
    "            new_fea.append(old_fea[idx])\n",
    "        else:\n",
    "            print(\"Filter feature:%s\" % old_fea[idx])\n",
    "    return new_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开搞！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org dataset = 496509\n",
      "0    78268\n",
      "3    71199\n",
      "1    70931\n",
      "2    68387\n",
      "4    68318\n",
      "5    63614\n",
      "6    57421\n",
      "7    18371\n",
      "Name: day, dtype: int64\n",
      "0    78268\n",
      "3    71199\n",
      "1    70931\n",
      "2    68387\n",
      "4    68318\n",
      "5    63614\n",
      "6    57421\n",
      "7    18371\n",
      "Name: day, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#这块一般不怎么需要改了，拿最后一天做验证\n",
    "#注意下面验证版和测试版，测试版是我们自己玩的，验证版是扔线上的，通过注释不同的跑就行了\n",
    "\n",
    "\n",
    "# %run BaseFrame.py\n",
    "% run ../util/time_series_split.py\n",
    "\n",
    "tmp_train_df = train_df.copy()#.fillna(-1)\n",
    "\n",
    "# tmp_train_df[\"user_gender_id*shop_review_positive_rate\"] = tmp_train_df[\"user_gender_id\"] * (1+tmp_train_df[\"shop_review_positive_rate\"])\n",
    "\n",
    "# tmp_train_df[\"user_gender_id*shop_star_level\"] = tmp_train_df[\"user_gender_id\"] * (1+tmp_train_df[\"shop_star_level\"])\n",
    "\n",
    "\n",
    "cols_tool.fit(train_df)\n",
    "\n",
    "categorical_one_hot_cols = cols_tool.get_onehoted_cols(\"categorical\")\n",
    "listype_one_hot_cols = cols_tool.get_onehoted_cols(\"listype\")\n",
    "\n",
    "index_col = \"instance_id\"\n",
    "\n",
    "\n",
    "categorical_cols = cols_tool.get_raw_categorical_cols()\n",
    "listype_cols = cols_tool.get_raw_listype_cols()\n",
    "numerical_cols = cols_tool.get_raw_numerical_cols()\n",
    "target_col = cols_tool.get_raw_target_col()\n",
    "\n",
    "\n",
    "print(\"org dataset = %s\" % len(tmp_train_df))\n",
    "print(tmp_train_df[\"day\"].value_counts())\n",
    "\n",
    "\n",
    "# tmp_train_df = featProc.balance_pos_neg_sample(tmp_train_df, (1,10))\n",
    "# print \"sampling dataset =\", len(tmp_train_df)\n",
    "# print tmp_train_df[\"day\"].value_counts()\n",
    "\n",
    "# 抽样\n",
    "# tmp_train_df = tmp_train_df.sample(n=10000, random_state=666)\n",
    "# print \"sampling dataset =\", len(tmp_train_df)\n",
    "\n",
    "print(tmp_train_df[\"day\"].value_counts())\n",
    "\n",
    "#测试版\n",
    "valid_df = tmp_train_df.loc[tmp_train_df[\"day\"]==6]\n",
    "tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]<6]\n",
    "# tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]>0]\n",
    "res_df=valid_df.copy()\n",
    "\n",
    "# #线上验证版\n",
    "# valid_df = tmp_train_df.loc[tmp_train_df[\"day\"]==6]\n",
    "# res_df = tmp_train_df.loc[tmp_train_df[\"day\"]==7]\n",
    "# tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]<=6]\n",
    "\n",
    "# print(\"res dataset = %s\" % len(res_df))\n",
    "# print(\"valid dateset = %s\" % len(valid_df))\n",
    "# print(\"train dateset = %s\" % len(tmp_train_df))\n",
    "\n",
    "# tmp_train_df = tmp_train_df[on_cols]\n",
    "# valid_df = valid_df[on_cols]\n",
    "\n",
    "\n",
    "# on_cols = [target_col]+numerical_cols+categorical_one_hot_cols+listype_one_hot_cols\n",
    "# # on_cols = [target_col]+numerical_cols\n",
    "# # +categorical_one_hot_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_lr(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"LR %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "\n",
    "# on_cols = [target_col]+categorical_cols+numerical_cols\n",
    "# # on_cols = [target_col]+select_best_features(tmp_train_df[on_cols], 0.8)\n",
    "# train_lls, test_lls = train_and_test_randomforest(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"XGBoost %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "\n",
    "# on_cols = [target_col]+categorical_cols+numerical_cols\n",
    "# on_cols = [target_col]+select_best_features(tmp_train_df[on_cols], 0.95)\n",
    "# train_lls, test_lls = train_and_test_randomforest(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"XGBoost %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# +listype_one_hot_cols\n",
    "# on_cols = ffea(on_cols)\n",
    "\n",
    "\n",
    "\n",
    "# on_cols = [target_col]+categorical_cols+numerical_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_randomforest(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"RandomForest %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "# on_cols = [target_col]+categorical_cols+numerical_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_gbdt(tmp_train_df[on_cols], valid_df[on_cols], res_df[on_cols+[index_col]])\n",
    "# print(\"GBDT %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n",
    "# train_lls, test_lls = train_and_test_combine(tmp_train_df[on_cols], valid_df[on_cols])\n",
    "# print \"Combine TrainLogLoss = %s | TestLogLoss = %s\" % (train_lls, test_lls)\n",
    "\n",
    "# on_cols = [target_col]+numerical_cols+categorical_one_hot_cols+listype_one_hot_cols\n",
    "# train_lls, test_lls = train_and_test_FFM(tmp_train_df, valid_df)\n",
    "# print \"FFM TrainLogLoss = %s | TestLogLoss = %s\" % (train_lls, test_lls)\n",
    "\n",
    "# train_lls, test_lls = train_and_test_FFM(tmp_train_df, valid_df, res_df)\n",
    "# print(\"LR %s dimension TrainLogLoss = %s | TestLogLoss = %s\" % (len(tmp_train_df[on_cols].columns), train_lls, test_lls))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_valid_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-02e311444a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_valid_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# valid_df = gen_valid_set_FULL(tmp_train_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtmp_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"day\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_valid_set' is not defined"
     ]
    }
   ],
   "source": [
    "tmp_train_df = train_df.copy()\n",
    "valid_df = gen_valid_set(tmp_train_df)\n",
    "# valid_df = gen_valid_set_FULL(tmp_train_df)\n",
    "\n",
    "tmp_train_df = tmp_train_df.loc[tmp_train_df[\"day\"]<6]\n",
    "res_df=valid_df.copy()\n",
    "\n",
    "\n",
    "print len(tmp_train_df),  len(valid_df)\n",
    "\n",
    "\n",
    "categorical_one_hot_cols = cols_tool.get_onehoted_cols(\"categorical\", tmp_train_df)\n",
    "listype_one_hot_cols = cols_tool.get_onehoted_cols(\"listype\", tmp_train_df)\n",
    "\n",
    "index_col = \"instance_id\"\n",
    "\n",
    "categorical_cols = cols_tool.get_raw_categorical_cols()\n",
    "listype_cols = cols_tool.get_raw_listype_cols()\n",
    "numerical_cols = cols_tool.get_raw_numerical_cols()\n",
    "target_col = cols_tool.get_raw_target_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(valid_df)\n",
    "print valid_df[\"is_trade\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "on_cols = [target_col]+numerical_cols+categorical_one_hot_cols#+listype_one_hot_cols\n",
    "# print on_cols\n",
    "train_lls, test_lls = train_and_test_lr(tmp_train_df[on_cols], valid_df[on_cols], None)\n",
    "print(\"LR TrainLogLoss = %s | TestLogLoss = %s\" % (train_lls, test_lls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = valid_df.copy()\n",
    "del X_test['is_trade']\n",
    "y_test = valid_df.copy()['is_trade']\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in xrange(1000000):\n",
    "    X_val_a, X_val_b, y_val_a, y_val_b = train_test_split(X_test, y_test, test_size=0.7, shuffle=True, random_state=i)\n",
    "    zero = []\n",
    "    for j in xrange(len(y_val_a)):\n",
    "        f = float('0.0000000000000%s' % random.randint(1,99))\n",
    "        zero.append(f)\n",
    "    lls = log_loss(y_val_a, zero)\n",
    "    llsdiff = abs(0.31284 - lls)\n",
    "    y_val_a = pd.DataFrame(y_val_a)\n",
    "    l = float(len(y_val_a))\n",
    "    \n",
    "    if llsdiff < 0.0001:\n",
    "        print i, \"[HIT]\", llsdiff, lls, len(y_val_a[y_val_a[\"is_trade\"] == 0])/l, len(y_val_a[y_val_a[\"is_trade\"] == 1])/l\n",
    "    else:\n",
    "        print i, llsdiff, lls, len(y_val_a[y_val_a[\"is_trade\"] == 0])/l, len(y_val_a[y_val_a[\"is_trade\"] == 1])/l\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = pd.read_table('/Users/yuhua/res1.csv',sep=' ')\n",
    "log_loss(a, t[\"predicted_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print cols_tool.numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiting ... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2659844bdb16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     fpfp.addLayeringOrgKeys(n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#一定要fit一个全集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfpfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfpfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoFFMData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"one_hot-train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuhua/先验CTR模型线上观察/Alimama比赛/src/yuhua/FeatureProcess.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m#收集listype的Onehot特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuhua/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuhua/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuhua/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   3101\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3103\u001b[0;31m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run ffmyh.py\n",
    "%run FeatureProcess.py\n",
    "\n",
    "fpfp = FeatureProcess(target=target, categorical=categorical, numerical=numerical, listype = listype)\n",
    "# for n in numerical:\n",
    "#     fpfp.addLayeringOrgKeys(n)\n",
    "#一定要fit一个全集\n",
    "fpfp.fit(train_df)\n",
    "\n",
    "fpfp.toFFMData(tmp_train_df, \"one_hot-train.txt\")\n",
    "fpfp.toFFMData(valid_df, \"one_hot-valid.txt\")\n",
    "# model = FFM(fpfp,\n",
    "#             reg_param = 0.00001,\n",
    "#             k = 4,\n",
    "#             iter_max = 500,\n",
    "#             learing_rate = 0.01,\n",
    "#             threads = 7,\n",
    "#             auto_stop = False,\n",
    "#             quiet = False)\n",
    "\n",
    "\n",
    "    \n",
    "# model.fit(tmp_train_df, valid_df)\n",
    "\n",
    "# train_lls = log_loss(train_df[\"is_trade\"],model.predict_proba(train_df))\n",
    "# test_lls = log_loss(test_df[\"is_trade\"],model.predict_proba(test_df))\n",
    "\n",
    "# print train_lls, test_lls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- | XGboost | 增减 | LR |增减\n",
    "- | :-: | :-: | :-: | :-:\n",
    "原始特征 | 0.0829350458274 | / | 0.0829428179401 | /\n",
    "原始特征+时间 | 0.0828004994761 | +++ | 0.082869098617 | +\n",
    "同上+按天计算交易率 | 0.0828218434014 | - | 0.0827914564667 | ++\n",
    "同上+predict_prop_cate相似度 | 0.082778758021 | ++ | 0.082798919764 | --\n",
    "同上+cate prop丰富度 | 0.0828356414143 | -- | 0.0828009736655 | -\n",
    "同上+cate/prop list OneHot | 0.0828356414143 | / | 0.0827979171146 | +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print tmp_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(SelectKBest(chi2,k=2).fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
